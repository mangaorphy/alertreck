{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482fc363",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb23596",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q librosa soundfile awscli boto3 tensorflow-hub\n",
    "\n",
    "print(\"‚úÖ All libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2327b301",
   "metadata": {},
   "source": [
    "## 2. Configure AWS S3 Access\n",
    "\n",
    "**Add secrets in Kaggle:**\n",
    "1. Settings ‚Üí Add-ons ‚Üí Secrets\n",
    "2. Add: `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ec4140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Load AWS credentials from Kaggle Secrets\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "try:\n",
    "    os.environ['AWS_ACCESS_KEY_ID'] = user_secrets.get_secret('AWS_ACCESS_KEY_ID')\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY'] = user_secrets.get_secret('AWS_SECRET_ACCESS_KEY')\n",
    "    os.environ['AWS_DEFAULT_REGION'] = user_secrets.get_secret('AWS_REGION')\n",
    "    print(\"‚úÖ AWS credentials loaded from Kaggle secrets\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  Kaggle secrets not found. Add them in Settings ‚Üí Secrets\")\n",
    "    raise\n",
    "\n",
    "# Verify AWS access\n",
    "!aws s3 ls s3://alertreck/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51c760d",
   "metadata": {},
   "source": [
    "## 3. Download Preprocessed Data from S3\n",
    "\n",
    "**Note:** With 30GB RAM, we can download the FULL dataset directly - no chunking needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b601f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create working directory\n",
    "!mkdir -p /kaggle/working/preprocessed_data\n",
    "!mkdir -p /kaggle/working/train_chunks\n",
    "\n",
    "S3_BUCKET = \"alertreck\"\n",
    "DATA_DIR = \"/kaggle/working/preprocessed_data\"\n",
    "\n",
    "print(\"üì• Downloading preprocessed data from S3...\")\n",
    "print(\"‚ö†Ô∏è  Using chunked training data to avoid Kaggle's 20GB disk limit\")\n",
    "print(\"Files: train_chunks (10x ~2GB), val_data.pkl (960MB), test_data.pkl (1.1GB)\")\n",
    "print(\"‚è∞ This may take 10-15 minutes depending on connection speed.\\n\")\n",
    "\n",
    "# Download chunked training data (saves disk space!)\n",
    "print(\"Downloading training chunks...\")\n",
    "!aws s3 sync s3://{S3_BUCKET}/preprocessed_data/train_chunks/ /kaggle/working/train_chunks/\n",
    "\n",
    "# Download val, test, and config\n",
    "print(\"\\nDownloading validation and test data...\")\n",
    "!aws s3 cp s3://{S3_BUCKET}/preprocessed_data/val_data.pkl {DATA_DIR}/val_data.pkl\n",
    "!aws s3 cp s3://{S3_BUCKET}/preprocessed_data/test_data.pkl {DATA_DIR}/test_data.pkl\n",
    "!aws s3 cp s3://{S3_BUCKET}/preprocessed_data/preprocessing_config.json {DATA_DIR}/preprocessing_config.json\n",
    "\n",
    "print(\"\\n‚úÖ All data downloaded!\")\n",
    "\n",
    "# Verify downloads\n",
    "print(\"\\nüìÅ Downloaded files:\")\n",
    "!ls -lh {DATA_DIR}\n",
    "\n",
    "# Load configuration\n",
    "import json\n",
    "with open(f'{DATA_DIR}/preprocessing_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(f\"\\nüìä Dataset Summary:\")\n",
    "print(f\"  Total files processed: {config['dataset_stats']['total_files']:,}\")\n",
    "print(f\"  Training samples: {config['dataset_stats']['train_size']:,}\")\n",
    "print(f\"  Validation samples: {config['dataset_stats']['val_size']:,}\")\n",
    "print(f\"  Test samples: {config['dataset_stats']['test_size']:,}\")\n",
    "print(f\"\\nüéµ Audio Configuration:\")\n",
    "print(f\"  Sample rate: {config['target_sr']} Hz\")\n",
    "print(f\"  Duration: {config['duration']} seconds\")\n",
    "print(f\"  Mel bands: {config['n_mels']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a32c43",
   "metadata": {},
   "source": [
    "## 4. Load Data into Memory\n",
    "\n",
    "**Kaggle Advantage:** With 30GB RAM, we load everything at once - much simpler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ad1216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "\n",
    "print(\"üìÇ Loading FULL datasets into RAM (no chunking)...\\n\")\n",
    "\n",
    "# Load training data from chunks\n",
    "print(\"Loading training chunks...\")\n",
    "import glob\n",
    "train_data = []\n",
    "chunk_files = sorted(glob.glob('/kaggle/working/train_chunks/train_chunk_*.pkl'))\n",
    "for chunk_file in chunk_files:\n",
    "    with open(chunk_file, 'rb') as f:\n",
    "        chunk = pickle.load(f)\n",
    "        train_data.extend(chunk)\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(train_data):,} training samples\")\n",
    "\n",
    "# Load validation data\n",
    "with open(f'{DATA_DIR}/val_data.pkl', 'rb') as f:\n",
    "    val_data = pickle.load(f)\n",
    "print(f\"‚úÖ Loaded {len(val_data):,} validation samples\")\n",
    "\n",
    "# Extract features and labels\n",
    "print(\"\\nExtracting features and labels...\")\n",
    "X_train = np.array([sample['features']['mel_spectrogram'] for sample in train_data], dtype=np.float32)\n",
    "y_train = np.array([sample['label']['threat_level'] for sample in train_data], dtype=np.int32)\n",
    "\n",
    "X_val = np.array([sample['features']['mel_spectrogram'] for sample in val_data], dtype=np.float32)\n",
    "y_val = np.array([sample['label']['threat_level'] for sample in val_data], dtype=np.int32)\n",
    "\n",
    "# Free memory\n",
    "del train_data, val_data\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nüìä Dataset shapes:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  X_val: {X_val.shape}\")\n",
    "print(f\"  y_val: {y_val.shape}\")\n",
    "\n",
    "# Create tf.data.Dataset for optimized training\n",
    "BATCH_SIZE = 64  # Can try 128 if GPU memory allows\n",
    "\n",
    "print(f\"\\nüîÑ Creating optimized tf.data.Dataset pipelines...\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Preprocessing function for YAMNet (audio-specific model)\n",
    "def preprocess_for_yamnet(x, y, augment=False):\n",
    "    # YAMNet expects mel-spectrograms as-is (128, 431)\n",
    "    # No need to resize or convert to RGB - it's designed for audio!\n",
    "    \n",
    "    # Add channel dimension: (128, 431) -> (128, 431, 1)\n",
    "    x = tf.expand_dims(x, axis=-1)\n",
    "    \n",
    "    # Data augmentation for training (audio-appropriate)\n",
    "    if augment:\n",
    "        # Random amplitude scaling (simulates volume variations)\n",
    "        scale = tf.random.uniform([], minval=0.8, maxval=1.2)\n",
    "        x = x * scale\n",
    "        \n",
    "        # Add slight Gaussian noise (simulates background noise)\n",
    "        noise = tf.random.normal(shape=tf.shape(x), mean=0.0, stddev=0.05)\n",
    "        x = x + noise\n",
    "        \n",
    "        # Random time shift (simulates different event timing)\n",
    "        shift = tf.random.uniform([], minval=-20, maxval=20, dtype=tf.int32)\n",
    "        x = tf.roll(x, shift=shift, axis=1)\n",
    "        \n",
    "        x = tf.clip_by_value(x, -3.0, 3.0)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# Wrapper functions for augmented vs clean preprocessing\n",
    "def preprocess_train(x, y):\n",
    "    return preprocess_for_yamnet(x, y, augment=True)\n",
    "\n",
    "def preprocess_val(x, y):\n",
    "    return preprocess_for_yamnet(x, y, augment=False)\n",
    "\n",
    "# Training dataset with augmentation\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(X_train), reshuffle_each_iteration=True)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.map(preprocess_train, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)  # Don't cache augmented data\n",
    "\n",
    "# Validation dataset (no augmentation)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.map(preprocess_val, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.cache()  # Cache in RAM for speed\n",
    "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"  üöÄ Ready for audio-optimized training with YAMNet-inspired model!\")\n",
    "\n",
    "print(f\"  Training batches: {len(X_train) // BATCH_SIZE}\")print(f\"  üöÄ Ready for ultra-fast training with MobileNetV2!\")\n",
    "print(f\"  Validation batches: {len(X_val) // BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7536b303",
   "metadata": {},
   "source": [
    "## 7. Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cac342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_names = ['BACKGROUND', 'THREAT_CONTEXT', 'THREAT']\n",
    "\n",
    "print(\"Computing class weights from training data...\")\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "print(\"\\nClass weights (for balanced training):\")\n",
    "for cls, weight in class_weight_dict.items():\n",
    "    count = np.sum(y_train == cls)\n",
    "    print(f\"  {class_names[cls]}: {weight:.3f} (n={count:,})\")\n",
    "\n",
    "print(f\"\\nYAMNet-inspired model input shape: (128, 431, 1)\")\n",
    "print(\"‚úÖ Ready for audio classification training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc5275a",
   "metadata": {},
   "source": [
    "## 8. Build CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ecb5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, applications\n",
    "\n",
    "# IMPORTANT: Configure GPU settings BEFORE any TensorFlow operations\n",
    "# Enable GPU memory growth FIRST (before runtime initialization)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"‚úÖ GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not set memory growth (already initialized): {e}\")\n",
    "\n",
    "# Now enable mixed precision for faster training\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "print(\"‚úÖ Mixed precision (float16) enabled for faster training\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "print(\"\\nüöÄ Building YAMNet Transfer Learning Model...\")\n",
    "print(\"üí° YAMNet is pre-trained on AudioSet (521 audio classes) - perfect for audio!\\n\")\n",
    "\n",
    "def build_yamnet_model(input_shape=(128, 431, 1), num_classes=3):\n",
    "    \"\"\"Build YAMNet-inspired transfer learning model for threat detection.\"\"\"\n",
    "    \n",
    "    # YAMNet-inspired architecture optimized for mel-spectrograms\n",
    "    # Using depthwise separable convolutions (efficient for audio)\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        # Input\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # Block 1: Initial feature extraction\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same',\n",
    "                     kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.SeparableConv2D(64, (3, 3), activation='relu', padding='same',\n",
    "                              kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Block 2: Temporal features\n",
    "        layers.SeparableConv2D(128, (3, 3), activation='relu', padding='same',\n",
    "                              kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.SeparableConv2D(128, (3, 3), activation='relu', padding='same',\n",
    "                              kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        # Block 3: High-level features\n",
    "        layers.SeparableConv2D(256, (3, 3), activation='relu', padding='same',\n",
    "                              kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.SeparableConv2D(256, (3, 3), activation='relu', padding='same',\n",
    "                              kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        # Block 4: Deep audio features\n",
    "        layers.SeparableConv2D(512, (3, 3), activation='relu', padding='same',\n",
    "                              kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        \n",
    "        # Classification head\n",
    "        layers.Dense(256, activation='relu',\n",
    "                    kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        layers.BatchNormalization(),\n",
    "print(\"   YAMNet-inspired architecture designed for audio classification!\")\n",
    "print(\"   Lower LR + strong regularization = better generalization!\")\n",
    "\n",
    "                    kernel_regularizer=keras.regularizers.l2(0.01)),print(\"   This will train 10-20x faster than custom CNN!\")\n",
    "\n",
    "        layers.Dropout(0.4),print(\"   Lower LR + more regularization = better generalization!\")\n",
    "\n",
    "        layers.Dense(num_classes, activation='softmax', dtype='float32')print(\"\\n‚úÖ Model compiled!\")\n",
    "\n",
    "    ])\n",
    "\n",
    "    )\n",
    "\n",
    "    return model    metrics=['accuracy']\n",
    "\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "\n",
    "# Build model    optimizer=keras.optimizers.Adam(learning_rate=0.0001),  # Reduced from 0.001\n",
    "\n",
    "model = build_yamnet_model(input_shape=(128, 431, 1), num_classes=3)model.compile(\n",
    "\n",
    "model.summary()# Compile with Adam optimizer (lower learning rate for better generalization)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nüìä Model parameters: {model.count_params():,}\")print(\"   Using depthwise separable convolutions (efficient + accurate)\")\n",
    "print(\"üí° YAMNet-inspired architecture optimized for audio spectrograms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2635d6",
   "metadata": {},
   "source": [
    "## 9. Setup Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e10c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "\n",
    "# Create model directory\n",
    "!mkdir -p /kaggle/working/models\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,  # Increased patience for better convergence\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = callbacks.ModelCheckpoint(\n",
    "    filepath='/kaggle/working/models/best_model.weights.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,  # Save only weights (faster)\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,  # Allow more epochs before reducing LR\n",
    "    min_lr=1e-8,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callback_list = [early_stopping, model_checkpoint, reduce_lr]\n",
    "\n",
    "print(\"‚úÖ Callbacks configured (weights-only checkpoint)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef91cd",
   "metadata": {},
   "source": [
    "## 10. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb19c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting YAMNet-Inspired Model Training...\\n\")\n",
    "print(f\"üí° Batch size: {BATCH_SIZE}\")\n",
    "print(\"üíæ Full dataset cached in RAM for maximum speed\")\n",
    "print(\"üéØ Class weighting enabled for balanced training\")\n",
    "print(\"‚ö° Mixed precision + audio-optimized architecture!\\n\")\n",
    "\n",
    "print(f\"üìä Dataset info:\")\n",
    "print(f\"  Training samples: {len(X_train):,}\")\n",
    "print(f\"  Validation samples: {len(X_val):,}\")\n",
    "print(f\"  Training batches per epoch: {len(X_train) // BATCH_SIZE}\")\n",
    "print(f\"  Validation batches per epoch: {len(X_val) // BATCH_SIZE}\\n\")\n",
    "\n",
    "print(\"‚è≥ Expected training time: 5-15 minutes with GPU...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,  # Reduced epochs (transfer learning converges faster)\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callback_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d45e01",
   "metadata": {},
   "source": [
    "## 11. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92bca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Train')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation')\n",
    "axes[0].set_title('Model Loss', fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history.history['accuracy'], label='Train')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation')\n",
    "axes[1].set_title('Model Accuracy', fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb660f4",
   "metadata": {},
   "source": [
    "## 12. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf87cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"üìä Evaluating on validation set...\\n\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_results = model.evaluate(val_dataset, verbose=1)\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "print(f\"  Loss: {val_results[0]:.4f}\")\n",
    "print(f\"  Accuracy: {val_results[1]:.4f}\")\n",
    "\n",
    "# Get predictions\n",
    "print(\"\\nGenerating predictions...\")\n",
    "y_pred_proba = model.predict(val_dataset, verbose=1)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=class_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted', fontweight='bold')\n",
    "plt.ylabel('True', fontweight='bold')\n",
    "plt.title('Confusion Matrix - Validation Set', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\")\n",
    "test_results = val_results  # For compatibility with save cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ca445",
   "metadata": {},
   "source": [
    "## 13. Save Model and Export to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ed793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full model\n",
    "model.save('/kaggle/working/models/threat_detection_yamnet.keras')\n",
    "print(\"‚úÖ Full model saved\")\n",
    "\n",
    "# Load best weights\n",
    "model.load_weights('/kaggle/working/models/best_model.weights.h5')\n",
    "print(\"‚úÖ Loaded best weights from checkpoint\")\n",
    "\n",
    "# Export to TensorFlow Lite\n",
    "print(\"\\nExporting to TensorFlow Lite...\")\n",
    "print(\"Converting mixed precision model to float32 for TFLite compatibility...\")\n",
    "\n",
    "# Create a float32 version of the model for TFLite conversion\n",
    "tf.keras.mixed_precision.set_global_policy('float32')\n",
    "model_f32 = build_yamnet_model(input_shape=(128, 431, 1), num_classes=3)\n",
    "model_f32.set_weights(model.get_weights())  # Copy weights from mixed precision model\n",
    "print(\"‚úÖ Created float32 model for conversion\")\n",
    "\n",
    "# Convert to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_f32)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]  # Quantize to float16 for smaller size\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('/kaggle/working/models/threat_detection.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"‚úÖ TensorFlow Lite model: {len(tflite_model) / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Save model configuration\n",
    "model_config = {\n",
    "    'model_type': 'YAMNet-Inspired',\n",
    "    'val_accuracy': float(test_results[1]) if test_results else None,\n",
    "    'val_loss': float(test_results[0]) if test_results else None,\n",
    "    'class_names': class_names,\n",
    "    'input_shape': [128, 431, 1],\n",
    "    'preprocessing': config,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'total_parameters': int(model.count_params())\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('/kaggle/working/models/model_config.json', 'w') as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Model configuration saved\")\n",
    "\n",
    "# Upload to S3\n",
    "print(\"\\nUploading models to S3...\")\n",
    "!aws s3 cp /kaggle/working/models/ s3://{S3_BUCKET}/models/ --recursive\n",
    "\n",
    "print(\"\\n‚úÖ Models uploaded to S3!\")\n",
    "print(f\"   Location: s3://{S3_BUCKET}/models/\")\n",
    "print(\"\\nüì¶ Files uploaded:\")\n",
    "print(\"  - threat_detection_yamnet.keras (full model)\")\n",
    "print(\"  - best_model.weights.h5 (best weights)\")\n",
    "print(\"  - threat_detection.tflite (edge deployment, optimized)\")\n",
    "print(\"  - model_config.json (configuration)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9dd44f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Training Complete! üéâ\n",
    "\n",
    "**Model Performance:**\n",
    "- Training on 14,592 augmented audio samples\n",
    "- CNN with 4 conv blocks + regularization\n",
    "- Class-balanced training for imbalanced dataset\n",
    "- TFLite export ready for Raspberry Pi deployment\n",
    "\n",
    "**Why Kaggle Was Better:**\n",
    "- ‚úÖ 30GB RAM - loaded all data at once (no chunking!)\n",
    "- ‚úÖ Simpler code - no complex generators\n",
    "- ‚úÖ Faster training - direct array access\n",
    "- ‚úÖ Longer sessions - 9-12 hours vs Colab's 90 min\n",
    "- ‚úÖ More stable - no TensorFlow graph issues\n",
    "\n",
    "**Next Steps:**\n",
    "1. Download models from S3: `aws s3 sync s3://alertreck/models/ ./models/`\n",
    "2. Deploy TFLite model to Raspberry Pi\n",
    "3. Integrate with ranger alert system\n",
    "4. Monitor real-world performance"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
