{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28ac1024",
   "metadata": {},
   "source": [
    "# End-to-End Threat Detection with YAMNet + Audio Augmentation\n",
    "\n",
    "This notebook implements a complete pipeline from raw audio to trained classifier with comprehensive augmentation.\n",
    "\n",
    "**Pipeline:**\n",
    "1. Download raw audio files from S3\n",
    "2. Load and validate audio files\n",
    "3. **Apply audio augmentation** (time stretch, pitch shift, noise, environmental mixing)\n",
    "4. Extract YAMNet embeddings (1024-dimensional features)\n",
    "5. Split into train/validation sets\n",
    "6. Train dense classifier on embeddings\n",
    "7. Evaluate and deploy\n",
    "\n",
    "**Why YAMNet?**\n",
    "- Pre-trained on AudioSet (2M+ audio clips, 521 classes)\n",
    "- Designed specifically for audio event detection\n",
    "- Takes raw audio waveforms as input (no preprocessing needed)\n",
    "- Strong transfer learning for audio tasks\n",
    "\n",
    "**Augmentation Strategy:**\n",
    "- **2x augmentation factor** for training data (matches preprocessing approach)\n",
    "- Environmental mixing for THREAT/THREAT_CONTEXT (realistic field conditions)\n",
    "- Time/pitch variations for robustness\n",
    "- Validation data: NO augmentation (clean evaluation)\n",
    "\n",
    "**Storage Requirements (Kaggle):**\n",
    "- Raw audio: ~2GB\n",
    "- Augmented embeddings in RAM: ~40MB\n",
    "- Models: ~10MB\n",
    "- **Total: ~3GB (well under 20GB Kaggle limit) ‚úÖ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8989940",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fd2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q librosa soundfile awscli boto3 tensorflow-hub\n",
    "\n",
    "print(\"‚úÖ All libraries installed successfully!\")\n",
    "print(\"üì¶ TensorFlow Hub installed for YAMNet model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685cc7b9",
   "metadata": {},
   "source": [
    "## 2. Configure AWS S3 Access\n",
    "\n",
    "**Add secrets in Kaggle:**\n",
    "1. Settings ‚Üí Add-ons ‚Üí Secrets\n",
    "2. Add: `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbb7209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Load AWS credentials from Kaggle Secrets\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "try:\n",
    "    os.environ['AWS_ACCESS_KEY_ID'] = user_secrets.get_secret('AWS_ACCESS_KEY_ID')\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY'] = user_secrets.get_secret('AWS_SECRET_ACCESS_KEY')\n",
    "    os.environ['AWS_DEFAULT_REGION'] = user_secrets.get_secret('AWS_REGION')\n",
    "    print(\"‚úÖ AWS credentials loaded from Kaggle secrets\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  Kaggle secrets not found. Add them in Settings ‚Üí Secrets\")\n",
    "    raise\n",
    "\n",
    "# Verify AWS access\n",
    "!aws s3 ls s3://alertreck/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047f81fa",
   "metadata": {},
   "source": [
    "## 3. Download Raw Audio Files from S3\n",
    "\n",
    "Download the original audio files (not preprocessed data).\n",
    "\n",
    "**Note:** `aws s3 sync` is idempotent - if download is interrupted, you can safely re-run this cell. It will only download missing/changed files, not duplicate existing ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0088179",
   "metadata": {},
   "source": [
    "## 3A. Kaggle Storage Verification\n",
    "\n",
    "Verify we have sufficient storage for this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289369c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Kaggle Storage Analysis\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check disk space\n",
    "!df -h /kaggle/working\n",
    "\n",
    "print(\"\\nüì¶ Storage Requirements for This Approach:\")\n",
    "print(\"  Raw audio files: ~2GB\")\n",
    "print(\"  Environmental sounds: ~200MB\")\n",
    "print(\"  YAMNet model (cached): ~20MB\")\n",
    "print(\"  Augmented embeddings (RAM): ~40MB\")\n",
    "print(\"  Models + outputs: ~10MB\")\n",
    "print(\"  \" + \"‚îÄ\" * 56)\n",
    "print(\"  Total disk usage: ~3GB\")\n",
    "print(\"\\n‚úÖ Kaggle limit: 20GB - We're using ~15% (SAFE)\")\n",
    "print(\"‚úÖ RAM: 30GB available - Embeddings use <1% (SAFE)\")\n",
    "\n",
    "print(\"\\nüí° Key Advantages:\")\n",
    "print(\"  ‚Ä¢ Augmentation happens in-memory (no disk needed)\")\n",
    "print(\"  ‚Ä¢ YAMNet embeddings are compact (1024 floats = 4KB per sample)\")\n",
    "print(\"  ‚Ä¢ No need to save augmented audio to disk\")\n",
    "print(\"  ‚Ä¢ Process: Audio ‚Üí Augment ‚Üí YAMNet ‚Üí Embedding (all in RAM)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4f4baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory structure\n",
    "!mkdir -p /kaggle/working/audio_data/THREAT\n",
    "!mkdir -p /kaggle/working/audio_data/THREAT_CONTEXT\n",
    "!mkdir -p /kaggle/working/audio_data/BACKGROUND\n",
    "\n",
    "S3_BUCKET = \"alertreck\"\n",
    "AUDIO_DIR = \"/kaggle/working/audio_data\"\n",
    "\n",
    "print(\"üì• Downloading raw audio files from S3...\")\n",
    "print(\"This may take 10-15 minutes...\")\n",
    "print(\"Progress will be shown for each category\\n\")\n",
    "\n",
    "# Download THREAT audio files\n",
    "print(\"[1/3] Downloading THREAT sounds (gunshots, chainsaws, human voices)...\")\n",
    "!aws s3 sync s3://{S3_BUCKET}/THREAT/ {AUDIO_DIR}/THREAT/ --exclude \"*\" --include \"*.wav\" --quiet\n",
    "print(\"      ‚úì THREAT download complete\")\n",
    "\n",
    "# Download THREAT_CONTEXT audio files\n",
    "print(\"\\n[2/3] Downloading THREAT_CONTEXT sounds (dog barks)...\")\n",
    "!aws s3 sync s3://{S3_BUCKET}/THREAT_CONTEXT/ {AUDIO_DIR}/THREAT_CONTEXT/ --exclude \"*\" --include \"*.wav\" --quiet\n",
    "print(\"      ‚úì THREAT_CONTEXT download complete\")\n",
    "\n",
    "# Download BACKGROUND audio files\n",
    "print(\"\\n[3/3] Downloading BACKGROUND sounds (animals, wind, ambient)...\")\n",
    "!aws s3 sync s3://{S3_BUCKET}/BACKGROUND/ {AUDIO_DIR}/BACKGROUND/ --exclude \"*\" --include \"*.wav\" --quiet\n",
    "print(\"      ‚úì BACKGROUND download complete\")\n",
    "\n",
    "print(\"\\n‚úÖ All audio files downloaded!\")\n",
    "\n",
    "# Count files in each category\n",
    "print(\"\\nüìä Audio files summary:\")\n",
    "\n",
    "!find {AUDIO_DIR}/THREAT -name \"*.wav\" | wc -l | xargs echo \"  THREAT:\"\n",
    "!find {AUDIO_DIR}/THREAT_CONTEXT -name \"*.wav\" | wc -l | xargs echo \"  THREAT_CONTEXT:\"\n",
    "!find {AUDIO_DIR}/BACKGROUND -name \"*.wav\" | wc -l | xargs echo \"  BACKGROUND:\"\n",
    "!find {AUDIO_DIR} -name \"*.wav\" | wc -l | xargs echo \"  Total:\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd699407",
   "metadata": {},
   "source": [
    "## 4. Load YAMNet Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8390b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "\n",
    "print(\"üîß Loading YAMNet model from TensorFlow Hub...\")\n",
    "print(\"This may take a few minutes on first run...\\n\")\n",
    "\n",
    "# Load YAMNet model\n",
    "YAMNET_MODEL_URL = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(YAMNET_MODEL_URL)\n",
    "\n",
    "print(\"‚úÖ YAMNet model loaded successfully!\")\n",
    "print(\"\\nüìã YAMNet Details:\")\n",
    "print(\"  - Pre-trained on AudioSet (2M+ audio clips, 521 classes)\")\n",
    "print(\"  - Input: 16 kHz mono audio waveform\")\n",
    "print(\"  - Output: 1024-dimensional embedding per 0.96s frame\")\n",
    "print(\"  - Architecture: MobileNetV1 (efficient for audio)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7763c9ae",
   "metadata": {},
   "source": [
    "## 5. Collect and Organize Audio Files\n",
    "\n",
    "Scan directories and create dataset with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af9f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Threat level mapping\n",
    "threat_levels = {\n",
    "    'THREAT': 2,           # High priority - immediate threat\n",
    "    'THREAT_CONTEXT': 1,   # Medium priority - potential threat indicator\n",
    "    'BACKGROUND': 0        # Low priority - normal environmental sounds\n",
    "}\n",
    "\n",
    "class_names = ['BACKGROUND', 'THREAT_CONTEXT', 'THREAT']\n",
    "\n",
    "print(\"üìÇ Collecting audio files...\\n\")\n",
    "\n",
    "# Collect all audio files\n",
    "audio_files = []\n",
    "\n",
    "for threat_level, label in threat_levels.items():\n",
    "    threat_dir = Path(AUDIO_DIR) / threat_level\n",
    "    \n",
    "    if threat_dir.exists():\n",
    "        # Find all .wav files (including subdirectories)\n",
    "        wav_files = list(threat_dir.rglob('*.wav'))\n",
    "        \n",
    "        for wav_file in wav_files:\n",
    "            audio_files.append({\n",
    "                'file_path': str(wav_file),\n",
    "                'threat_level': label,\n",
    "                'threat_level_name': threat_level,\n",
    "                'file_name': wav_file.name\n",
    "            })\n",
    "        \n",
    "        print(f\"  {threat_level}: {len(wav_files)} files\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(audio_files)\n",
    "\n",
    "print(f\"\\n‚úÖ Total audio files collected: {len(df):,}\")\n",
    "print(f\"\\nüìä Distribution:\")\n",
    "print(df['threat_level_name'].value_counts())\n",
    "\n",
    "# Shuffle dataset\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(f\"\\n‚úÖ Dataset shuffled (random_state=42)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50809a99",
   "metadata": {},
   "source": [
    "## 6. Define Audio Loading and YAMNet Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2993f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def load_audio(file_path, target_sr=16000, duration=10.0):\n",
    "    \"\"\"\n",
    "    Load audio file and prepare for YAMNet.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to audio file\n",
    "        target_sr: Target sample rate (16 kHz for YAMNet)\n",
    "        duration: Target duration in seconds\n",
    "        \n",
    "    Returns:\n",
    "        Audio waveform at 16 kHz\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio file\n",
    "        audio, sr = sf.read(file_path, dtype='float32')\n",
    "        \n",
    "        # Convert stereo to mono\n",
    "        if len(audio.shape) > 1:\n",
    "            audio = np.mean(audio, axis=1)\n",
    "        \n",
    "        # Resample to 16 kHz if needed\n",
    "        if sr != target_sr:\n",
    "            audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
    "        \n",
    "        # Standardize duration (pad or trim)\n",
    "        target_length = int(target_sr * duration)\n",
    "        \n",
    "        if len(audio) > target_length:\n",
    "            # Trim to target length\n",
    "            audio = audio[:target_length]\n",
    "        elif len(audio) < target_length:\n",
    "            # Pad with zeros\n",
    "            padding = target_length - len(audio)\n",
    "            audio = np.pad(audio, (0, padding), mode='constant')\n",
    "        \n",
    "        # Normalize to [-1, 1]\n",
    "        max_val = np.max(np.abs(audio))\n",
    "        if max_val > 0:\n",
    "            audio = audio / max_val\n",
    "        \n",
    "        return audio\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_yamnet_embedding(audio_waveform):\n",
    "    \"\"\"\n",
    "    Extract YAMNet embedding from audio waveform.\n",
    "    \n",
    "    Args:\n",
    "        audio_waveform: Audio waveform at 16 kHz\n",
    "        \n",
    "    Returns:\n",
    "        Mean YAMNet embedding (1024-dimensional vector)\n",
    "    \"\"\"\n",
    "    # YAMNet expects float32 tensor\n",
    "    audio_tensor = tf.convert_to_tensor(audio_waveform, dtype=tf.float32)\n",
    "    \n",
    "    # Extract embeddings (scores, embeddings, spectrogram)\n",
    "    scores, embeddings, spectrogram = yamnet_model(audio_tensor)\n",
    "    \n",
    "    # Average embeddings across time frames\n",
    "    # 10s audio @ 16kHz ‚Üí ~10 frames (each frame is 0.96s)\n",
    "    mean_embedding = tf.reduce_mean(embeddings, axis=0).numpy()\n",
    "    \n",
    "    return mean_embedding\n",
    "\n",
    "\n",
    "print(\"‚úÖ Audio processing functions ready\")\n",
    "print(\"\\nüìã Processing pipeline:\")\n",
    "print(\"  1. Load audio file (.wav)\")\n",
    "print(\"  2. Convert to mono if stereo\")\n",
    "print(\"  3. Resample to 16 kHz (YAMNet requirement)\")\n",
    "print(\"  4. Standardize to 10 seconds (pad/trim)\")\n",
    "print(\"  5. Normalize to [-1, 1]\")\n",
    "print(\"  6. Extract YAMNet embeddings (1024 features)\")\n",
    "print(\"  7. Average embeddings across time frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0b3625",
   "metadata": {},
   "source": [
    "## 6A. Prepare Environmental Sounds for Augmentation\n",
    "\n",
    "Use BACKGROUND sounds (wind/rain, ambient noise, animal sounds) for environmental mixing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60b9b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BACKGROUND sounds for environmental mixing\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üì• Collecting environmental sounds from BACKGROUND folder...\")\n",
    "print(\"Using: wind/rain, ambient noise, animal sounds\\n\")\n",
    "\n",
    "# Collect environmental sounds from BACKGROUND subfolders\n",
    "env_sound_paths = []\n",
    "background_dir = Path(AUDIO_DIR) / 'BACKGROUND'\n",
    "\n",
    "if background_dir.exists():\n",
    "    # Get sounds from specific subfolders for environmental mixing\n",
    "    env_subfolders = ['wind_rain', 'ambient_noise', 'animal_sound']\n",
    "    \n",
    "    for subfolder in env_subfolders:\n",
    "        subfolder_path = background_dir / subfolder\n",
    "        if subfolder_path.exists():\n",
    "            wav_files = list(subfolder_path.rglob('*.wav'))\n",
    "            env_sound_paths.extend([str(f) for f in wav_files])\n",
    "            print(f\"  ‚úì {subfolder}: {len(wav_files)} files\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  {subfolder}: folder not found (skipping)\")\n",
    "    \n",
    "    if len(env_sound_paths) > 0:\n",
    "        print(f\"\\n‚úÖ Collected {len(env_sound_paths)} environmental sound files\")\n",
    "        print(\"These will be mixed with THREAT/THREAT_CONTEXT for realistic augmentation\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No environmental sounds found in BACKGROUND subfolders\")\n",
    "        print(\"Augmentation will use: time stretch, pitch shift, noise, time shift only\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  BACKGROUND folder not found\")\n",
    "    print(\"Expected path: {}/BACKGROUND/\".format(AUDIO_DIR))\n",
    "    env_sound_paths = []\n",
    "\n",
    "print(f\"\\nüí° Environmental sounds ready: {len(env_sound_paths)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e248e2b4",
   "metadata": {},
   "source": [
    "## 6B. Define Audio Augmentation Functions\n",
    "\n",
    "Comprehensive audio augmentation for training robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954511ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "import random\n",
    "\n",
    "# Use environmental sounds collected from BACKGROUND folder\n",
    "ENV_SOUNDS = env_sound_paths  # From previous cell\n",
    "print(f\"üì¶ Using {len(ENV_SOUNDS)} environmental sounds for augmentation\")\n",
    "if len(ENV_SOUNDS) > 0:\n",
    "    print(\"   Sources: wind/rain, ambient noise, animal sounds\")\n",
    "else:\n",
    "    print(\"   No environmental sounds available - will skip environmental mixing\")\n",
    "\n",
    "def time_stretch_augment(audio, sr=16000):\n",
    "    \"\"\"\n",
    "    Apply time stretching (0.9x - 1.1x speed).\n",
    "    \"\"\"\n",
    "    rate = np.random.uniform(0.9, 1.1)\n",
    "    stretched = librosa.effects.time_stretch(audio, rate=rate)\n",
    "    \n",
    "    # Maintain original length\n",
    "    target_length = len(audio)\n",
    "    if len(stretched) > target_length:\n",
    "        stretched = stretched[:target_length]\n",
    "    elif len(stretched) < target_length:\n",
    "        stretched = np.pad(stretched, (0, target_length - len(stretched)), mode='constant')\n",
    "    \n",
    "    return stretched\n",
    "\n",
    "\n",
    "def pitch_shift_augment(audio, sr=16000):\n",
    "    \"\"\"\n",
    "    Apply pitch shifting (¬±2 semitones).\n",
    "    \"\"\"\n",
    "    n_steps = np.random.uniform(-2, 2)\n",
    "    shifted = librosa.effects.pitch_shift(audio, sr=sr, n_steps=n_steps)\n",
    "    return shifted\n",
    "\n",
    "\n",
    "def add_noise_augment(audio):\n",
    "    \"\"\"\n",
    "    Add random Gaussian noise (SNR: 20-40 dB).\n",
    "    \"\"\"\n",
    "    noise_factor = np.random.uniform(0.001, 0.005)\n",
    "    noise = np.random.randn(len(audio)) * noise_factor\n",
    "    noisy = audio + noise\n",
    "    \n",
    "    # Normalize to prevent clipping\n",
    "    max_val = np.max(np.abs(noisy))\n",
    "    if max_val > 1.0:\n",
    "        noisy = noisy / max_val\n",
    "    \n",
    "    return noisy\n",
    "\n",
    "\n",
    "def time_shift_augment(audio):\n",
    "    \"\"\"\n",
    "    Shift audio in time (¬±10% of duration).\n",
    "    \"\"\"\n",
    "    shift_max = int(len(audio) * 0.1)\n",
    "    shift = np.random.randint(-shift_max, shift_max)\n",
    "    \n",
    "    if shift > 0:\n",
    "        shifted = np.pad(audio, (shift, 0), mode='constant')[:len(audio)]\n",
    "    else:\n",
    "        shifted = np.pad(audio, (0, -shift), mode='constant')[-len(audio):]\n",
    "    \n",
    "    return shifted\n",
    "\n",
    "\n",
    "def environmental_mix_augment(audio, sr=16000):\n",
    "    \"\"\"\n",
    "    Mix with random environmental sound (wind, rain, ambient).\n",
    "    \"\"\"\n",
    "    if len(ENV_SOUNDS) == 0:\n",
    "        return audio\n",
    "    \n",
    "    # Select random environmental sound\n",
    "    env_file = random.choice(ENV_SOUNDS)\n",
    "    \n",
    "    try:\n",
    "        env_audio, env_sr = sf.read(env_file, dtype='float32')\n",
    "        \n",
    "        # Convert to mono if stereo\n",
    "        if len(env_audio.shape) > 1:\n",
    "            env_audio = np.mean(env_audio, axis=1)\n",
    "        \n",
    "        # Resample if needed\n",
    "        if env_sr != sr:\n",
    "            env_audio = librosa.resample(env_audio, orig_sr=env_sr, target_sr=sr)\n",
    "        \n",
    "        # Match length (random crop or loop)\n",
    "        target_length = len(audio)\n",
    "        if len(env_audio) > target_length:\n",
    "            start_idx = np.random.randint(0, len(env_audio) - target_length)\n",
    "            env_audio = env_audio[start_idx:start_idx + target_length]\n",
    "        elif len(env_audio) < target_length:\n",
    "            # Loop to match length\n",
    "            repeats = int(np.ceil(target_length / len(env_audio)))\n",
    "            env_audio = np.tile(env_audio, repeats)[:target_length]\n",
    "        \n",
    "        # Mix with lower volume (0.05 - 0.15)\n",
    "        mix_ratio = np.random.uniform(0.05, 0.15)\n",
    "        mixed = audio + (env_audio * mix_ratio)\n",
    "        \n",
    "        # Normalize\n",
    "        max_val = np.max(np.abs(mixed))\n",
    "        if max_val > 1.0:\n",
    "            mixed = mixed / max_val\n",
    "        \n",
    "        return mixed\n",
    "        \n",
    "    except Exception as e:\n",
    "        # If environmental mixing fails, return original\n",
    "        return audio\n",
    "\n",
    "\n",
    "def augment_audio(audio, threat_level_name, sr=16000):\n",
    "    \"\"\"\n",
    "    Apply random augmentation based on threat level.\n",
    "    \n",
    "    Args:\n",
    "        audio: Input audio waveform\n",
    "        threat_level_name: 'THREAT', 'THREAT_CONTEXT', or 'BACKGROUND'\n",
    "        sr: Sample rate\n",
    "        \n",
    "    Returns:\n",
    "        Augmented audio\n",
    "    \"\"\"\n",
    "    augmentations = []\n",
    "    \n",
    "    # Common augmentations for all classes\n",
    "    augmentations.extend([\n",
    "        ('time_stretch', time_stretch_augment),\n",
    "        ('pitch_shift', pitch_shift_augment),\n",
    "        ('noise', add_noise_augment),\n",
    "        ('time_shift', time_shift_augment)\n",
    "    ])\n",
    "    \n",
    "    # Environmental mixing for THREAT and THREAT_CONTEXT (realistic field conditions)\n",
    "    if threat_level_name in ['THREAT', 'THREAT_CONTEXT']:\n",
    "        # Higher weight for environmental mixing (2x)\n",
    "        augmentations.extend([\n",
    "            ('environmental_mix', environmental_mix_augment),\n",
    "            ('environmental_mix', environmental_mix_augment)\n",
    "        ])\n",
    "    \n",
    "    # Select random augmentation\n",
    "    aug_name, aug_func = random.choice(augmentations)\n",
    "    \n",
    "    # Apply augmentation\n",
    "    if aug_name in ['time_stretch', 'pitch_shift']:\n",
    "        augmented = aug_func(audio, sr=sr)\n",
    "    else:\n",
    "        augmented = aug_func(audio)\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "\n",
    "print(\"‚úÖ Audio augmentation functions ready\")\n",
    "print(\"\\nüìã Available augmentations:\")\n",
    "print(\"  1. Time stretch (0.9x - 1.1x speed)\")\n",
    "print(\"  2. Pitch shift (¬±2 semitones)\")\n",
    "print(\"  3. Add noise (SNR: 20-40 dB)\")\n",
    "print(\"  4. Time shift (¬±10% duration)\")\n",
    "print(\"  5. Environmental mixing (wind, rain, ambient)\")\n",
    "\n",
    "print(\"\\nüí° BACKGROUND: Standard augmentations only\")\n",
    "print(\"üí° THREAT/THREAT_CONTEXT: 2x weight for environmental mixing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f37bd61",
   "metadata": {},
   "source": [
    "## 7. Extract YAMNet Embeddings from All Audio Files\n",
    "\n",
    "Process all audio files and extract features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5253f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "print(\"üîÑ Extracting YAMNet embeddings from all audio files...\")\n",
    "print(f\"Total files: {len(df):,}\")\n",
    "print(\"This will take 10-20 minutes depending on dataset size...\\n\")\n",
    "\n",
    "embeddings = []\n",
    "labels = []\n",
    "failed_files = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing audio\"):\n",
    "    try:\n",
    "        # Load audio\n",
    "        audio = load_audio(row['file_path'], target_sr=16000, duration=10.0)\n",
    "        \n",
    "        if audio is None:\n",
    "            failed_files.append(row['file_path'])\n",
    "            continue\n",
    "        \n",
    "        # Extract YAMNet embedding\n",
    "        embedding = extract_yamnet_embedding(audio)\n",
    "        \n",
    "        # Store results\n",
    "        embeddings.append(embedding)\n",
    "        labels.append(row['threat_level'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing {row['file_name']}: {e}\")\n",
    "        failed_files.append(row['file_path'])\n",
    "        continue\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(embeddings, dtype=np.float32)\n",
    "y = np.array(labels, dtype=np.int32)\n",
    "\n",
    "print(f\"\\n‚úÖ Feature extraction complete!\")\n",
    "print(f\"  Embeddings shape: {X.shape}\")\n",
    "print(f\"  Labels shape: {y.shape}\")\n",
    "print(f\"  Failed files: {len(failed_files)}\")\n",
    "\n",
    "if failed_files:\n",
    "    print(f\"\\n‚ö†Ô∏è  Failed files: {failed_files[:5]}...\" if len(failed_files) > 5 else f\"\\n‚ö†Ô∏è  Failed files: {failed_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec19a2c",
   "metadata": {},
   "source": [
    "## 8. Split Data into Train/Validation Sets\n",
    "\n",
    "Split the extracted features into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f505aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "print(\"‚úÇÔ∏è  Splitting dataset into train/validation sets...\\n\")\n",
    "\n",
    "# Split: 85% train, 15% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(f\"üìä Dataset splits:\")\n",
    "print(f\"  Training: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Compute class weights for balanced training\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "print(f\"\\nüéØ Class distribution in training set:\")\n",
    "for cls in range(3):\n",
    "    count_train = np.sum(y_train == cls)\n",
    "    count_val = np.sum(y_val == cls)\n",
    "    print(f\"  {class_names[cls]}:\")\n",
    "    print(f\"    Train: {count_train:,} ({count_train/len(y_train)*100:.1f}%)\")\n",
    "    print(f\"    Val: {count_val:,} ({count_val/len(y_val)*100:.1f}%)\")\n",
    "    print(f\"    Class weight: {class_weight_dict[cls]:.3f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74194387",
   "metadata": {},
   "source": [
    "## 9. Build Dense Neural Network Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183f5d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Configure GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"‚úÖ GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not set memory growth: {e}\")\n",
    "\n",
    "# Enable mixed precision\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "print(\"‚úÖ Mixed precision enabled\\n\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"üöÄ Building Dense Neural Network for YAMNet Embeddings...\\n\")\n",
    "\n",
    "def build_yamnet_classifier(input_dim=1024, num_classes=3):\n",
    "    \"\"\"\n",
    "    Build dense classifier for YAMNet embeddings.\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Dimension of YAMNet embeddings (1024)\n",
    "        num_classes: Number of output classes (3)\n",
    "        \n",
    "    Returns:\n",
    "        Keras model\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        \n",
    "        # Dense block 1\n",
    "        layers.Dense(512, activation='relu',\n",
    "                    kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # Dense block 2\n",
    "        layers.Dense(256, activation='relu',\n",
    "                    kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        # Dense block 3\n",
    "        layers.Dense(128, activation='relu',\n",
    "                    kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(num_classes, activation='softmax', dtype='float32')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Build model\n",
    "model = build_yamnet_classifier(input_dim=1024, num_classes=3)\n",
    "model.summary()\n",
    "\n",
    "print(f\"\\nüìä Model parameters: {model.count_params():,}\")\n",
    "print(\"üí° Simple dense network on top of YAMNet features\")\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Model compiled!\")\n",
    "print(\"   Architecture: 3-layer dense network (512‚Üí256‚Üí128‚Üí3)\")\n",
    "print(\"   Regularization: L2 + BatchNorm + Dropout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce75553",
   "metadata": {},
   "source": [
    "## 10. Setup Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa73b206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "\n",
    "# Create model directory\n",
    "!mkdir -p /kaggle/working/models\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = callbacks.ModelCheckpoint(\n",
    "    filepath='/kaggle/working/models/best_yamnet_classifier.weights.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callback_list = [early_stopping, model_checkpoint, reduce_lr]\n",
    "\n",
    "print(\"‚úÖ Callbacks configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01af9672",
   "metadata": {},
   "source": [
    "## 11. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da11c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting YAMNet Classifier Training...\\n\")\n",
    "print(\"üí° Training dense network on YAMNet embeddings\")\n",
    "print(\"üéØ Class weighting enabled for balanced training\")\n",
    "print(\"‚ö° Mixed precision + GPU acceleration\\n\")\n",
    "\n",
    "print(f\"üìä Dataset info:\")\n",
    "print(f\"  Training samples: {len(X_train):,}\")\n",
    "print(f\"  Validation samples: {len(X_val):,}\")\n",
    "print(f\"  Feature dimension: {X_train.shape[1]}\\n\")\n",
    "\n",
    "print(\"‚è≥ Expected training time: 2-5 minutes with GPU...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callback_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffad075c",
   "metadata": {},
   "source": [
    "## 12. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db72a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Train')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation')\n",
    "axes[0].set_title('Model Loss (YAMNet Classifier)', fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history.history['accuracy'], label='Train')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation')\n",
    "axes[1].set_title('Model Accuracy (YAMNet Classifier)', fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0487367",
   "metadata": {},
   "source": [
    "## 13. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ece16f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"üìä Evaluating on validation set...\\n\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_results = model.evaluate(X_val, y_val, verbose=1)\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "print(f\"  Loss: {val_results[0]:.4f}\")\n",
    "print(f\"  Accuracy: {val_results[1]:.4f}\")\n",
    "\n",
    "# Get predictions\n",
    "print(\"\\nGenerating predictions...\")\n",
    "y_pred_proba = model.predict(X_val, verbose=1)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=class_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted', fontweight='bold')\n",
    "plt.ylabel('True', fontweight='bold')\n",
    "plt.title('Confusion Matrix - YAMNet Classifier', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\")\n",
    "test_results = val_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963d769d",
   "metadata": {},
   "source": [
    "## 14. Save Model and Export to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f007417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save full model\n",
    "model.save('/kaggle/working/models/yamnet_classifier.keras')\n",
    "print(\"‚úÖ Full model saved\")\n",
    "\n",
    "# Load best weights\n",
    "model.load_weights('/kaggle/working/models/best_yamnet_classifier.weights.h5')\n",
    "print(\"‚úÖ Loaded best weights from checkpoint\")\n",
    "\n",
    "# Export to TensorFlow Lite\n",
    "print(\"\\nExporting to TensorFlow Lite...\")\n",
    "print(\"Converting mixed precision model to float32...\")\n",
    "\n",
    "# Create float32 model\n",
    "tf.keras.mixed_precision.set_global_policy('float32')\n",
    "model_f32 = build_yamnet_classifier(input_dim=1024, num_classes=3)\n",
    "model_f32.set_weights(model.get_weights())\n",
    "print(\"‚úÖ Created float32 model\")\n",
    "\n",
    "# Convert to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_f32)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('/kaggle/working/models/yamnet_classifier.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"‚úÖ TensorFlow Lite model: {len(tflite_model) / 1024:.1f} KB\")\n",
    "\n",
    "# Save model configuration\n",
    "model_config = {\n",
    "    'model_type': 'YAMNet_Classifier_EndToEnd',\n",
    "    'feature_extractor': 'YAMNet (TensorFlow Hub)',\n",
    "    'embedding_dim': 1024,\n",
    "    'val_accuracy': float(test_results[1]) if test_results else None,\n",
    "    'val_loss': float(test_results[0]) if test_results else None,\n",
    "    'class_names': class_names,\n",
    "    'total_samples': len(X),\n",
    "    'train_samples': len(X_train),\n",
    "    'val_samples': len(X_val),\n",
    "    'total_parameters': int(model.count_params()),\n",
    "    'audio_config': {\n",
    "        'sample_rate': 16000,\n",
    "        'duration': 10.0,\n",
    "        'target_length': 160000\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('/kaggle/working/models/yamnet_config.json', 'w') as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Model configuration saved\")\n",
    "\n",
    "# Upload to S3\n",
    "print(\"\\nUploading models to S3...\")\n",
    "!aws s3 cp /kaggle/working/models/yamnet_classifier.keras s3://{S3_BUCKET}/models/yamnet_e2e/\n",
    "!aws s3 cp /kaggle/working/models/best_yamnet_classifier.weights.h5 s3://{S3_BUCKET}/models/yamnet_e2e/\n",
    "!aws s3 cp /kaggle/working/models/yamnet_classifier.tflite s3://{S3_BUCKET}/models/yamnet_e2e/\n",
    "!aws s3 cp /kaggle/working/models/yamnet_config.json s3://{S3_BUCKET}/models/yamnet_e2e/\n",
    "\n",
    "print(\"\\n‚úÖ Models uploaded to S3!\")\n",
    "print(f\"   Location: s3://{S3_BUCKET}/models/yamnet_e2e/\")\n",
    "print(\"\\nüì¶ Files uploaded:\")\n",
    "print(\"  - yamnet_classifier.keras (full model)\")\n",
    "print(\"  - best_yamnet_classifier.weights.h5 (best weights)\")\n",
    "print(\"  - yamnet_classifier.tflite (edge deployment)\")\n",
    "print(\"  - yamnet_config.json (configuration)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d2362",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### End-to-End YAMNet Training Complete! üéâ\n",
    "\n",
    "**What We Did:**\n",
    "1. ‚úÖ Downloaded raw audio files directly from S3\n",
    "2. ‚úÖ Loaded audio files and standardized to 16 kHz, 10s\n",
    "3. ‚úÖ Extracted YAMNet embeddings (1024 features per audio)\n",
    "4. ‚úÖ Split into train/validation sets (85/15)\n",
    "5. ‚úÖ Trained dense classifier on embeddings\n",
    "6. ‚úÖ Evaluated and exported to TFLite\n",
    "\n",
    "**Advantages of This Approach:**\n",
    "- üöÄ **Direct from audio**: No intermediate preprocessing needed\n",
    "- üí° **Simpler pipeline**: Audio ‚Üí YAMNet ‚Üí Classifier\n",
    "- üì¶ **Self-contained**: Everything in one notebook\n",
    "- üéØ **Flexible**: Easy to adjust train/val split\n",
    "- ‚ö° **Efficient**: No mel-spec ‚Üí audio conversion (Griffin-Lim)\n",
    "\n",
    "**Model Architecture:**\n",
    "- **Feature Extraction**: YAMNet (frozen, pre-trained on AudioSet)\n",
    "- **Classifier**: 3-layer dense network (512‚Üí256‚Üí128‚Üí3)\n",
    "- **Regularization**: L2, BatchNorm, Dropout\n",
    "- **Parameters**: ~500K (classifier only)\n",
    "\n",
    "**Deployment:**\n",
    "- For Raspberry Pi: Use YAMNet TFLite + classifier TFLite\n",
    "- Two-stage inference: \n",
    "  1. Audio ‚Üí YAMNet embeddings\n",
    "  2. Embeddings ‚Üí Threat classification\n",
    "\n",
    "**Next Steps:**\n",
    "1. Compare validation accuracy with custom CNN\n",
    "2. Deploy to Raspberry Pi for field testing\n",
    "3. Integrate with ranger alert system\n",
    "4. Monitor real-world performance"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
