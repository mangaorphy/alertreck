{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54519908",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73cb10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install audio processing and AWS libraries\n",
    "!pip install -q librosa soundfile tqdm\n",
    "!pip install -q awscli boto3\n",
    "!pip install -q tensorflow scikit-learn\n",
    "\n",
    "print(\"‚úÖ All libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17d0569",
   "metadata": {},
   "source": [
    "## 2. Configure AWS S3 Access\n",
    "\n",
    "**Important:** You need your AWS credentials:\n",
    "- AWS Access Key ID\n",
    "- AWS Secret Access Key\n",
    "- AWS Region (e.g., us-east-1)\n",
    "\n",
    "‚ö†Ô∏è **Security Note:** Never hardcode credentials in notebooks. Use Colab secrets or environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38649cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Method 1: Use Colab Secrets (Recommended)\n",
    "# Add secrets in: Settings (üîë) > Secrets\n",
    "# Create: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION\n",
    "\n",
    "try:\n",
    "    os.environ['AWS_ACCESS_KEY_ID'] = userdata.get('AWS_ACCESS_KEY_ID')\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY'] = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
    "    os.environ['AWS_DEFAULT_REGION'] = userdata.get('AWS_REGION')\n",
    "    print(\"‚úÖ AWS credentials loaded from Colab secrets\")\n",
    "except:\n",
    "    # Method 2: Manual input (less secure)\n",
    "    print(\"‚ö†Ô∏è  Colab secrets not found. Using manual input.\")\n",
    "    from getpass import getpass\n",
    "    os.environ['AWS_ACCESS_KEY_ID'] = getpass('Enter AWS Access Key ID: ')\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY'] = getpass('Enter AWS Secret Access Key: ')\n",
    "    os.environ['AWS_DEFAULT_REGION'] = input('Enter AWS Region (e.g., us-east-1): ')\n",
    "    print(\"‚úÖ AWS credentials configured\")\n",
    "\n",
    "# Verify AWS CLI is configured\n",
    "!aws s3 ls s3://alertreck/ --no-sign-request 2>/dev/null || aws s3 ls s3://alertreck/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd1b9a5",
   "metadata": {},
   "source": [
    "## 3. Download Preprocessed Data from S3\n",
    "\n",
    "**Note:** The preprocessing has already been completed locally with environmental augmentation. This downloads the ready-to-use preprocessed data (~20GB) to skip the 2+ hour preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba37fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create working directory\n",
    "!mkdir -p /content/preprocessed_data\n",
    "!mkdir -p /content/data_chunks\n",
    "\n",
    "# S3 bucket configuration\n",
    "S3_BUCKET = \"alertreck\"\n",
    "PREPROCESSED_DIR = \"/content/preprocessed_data\"\n",
    "\n",
    "print(\"üì• Downloading preprocessed data from S3...\")\n",
    "print(\"Files: train chunks (10x ~2GB), val_data.pkl (960MB), test_data.pkl (1.1GB)\")\n",
    "print(\"‚è∞ This may take 5-10 minutes depending on connection speed.\\n\")\n",
    "\n",
    "# Download validation and test data (small enough to load directly)\n",
    "print(\"Downloading validation and test data...\")\n",
    "!aws s3 cp s3://{S3_BUCKET}/preprocessed_data/val_data.pkl {PREPROCESSED_DIR}/val_data.pkl\n",
    "!aws s3 cp s3://{S3_BUCKET}/preprocessed_data/test_data.pkl {PREPROCESSED_DIR}/test_data.pkl\n",
    "!aws s3 cp s3://{S3_BUCKET}/preprocessed_data/preprocessing_config.json {PREPROCESSED_DIR}/preprocessing_config.json\n",
    "\n",
    "print(\"\\n‚úÖ Val and test data downloaded!\")\n",
    "\n",
    "# Download pre-split training chunks (MUCH safer for memory!)\n",
    "print(\"\\nüì¶ Downloading pre-split training chunks...\")\n",
    "print(\"üí° Chunks were split locally to avoid Colab memory issues\\n\")\n",
    "!aws s3 sync s3://{S3_BUCKET}/preprocessed_data/train_chunks/ /content/data_chunks/\n",
    "\n",
    "print(\"\\n‚úÖ All chunks downloaded!\")\n",
    "\n",
    "# Verify downloads\n",
    "print(\"\\nüìÅ Downloaded files:\")\n",
    "!ls -lh {PREPROCESSED_DIR}\n",
    "print(\"\\nüì¶ Training chunks:\")\n",
    "!ls -lh /content/data_chunks/\n",
    "\n",
    "# Load configuration\n",
    "import json\n",
    "with open(f'{PREPROCESSED_DIR}/preprocessing_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Count chunks\n",
    "import glob\n",
    "chunk_files = glob.glob('/content/data_chunks/train_chunk_*.pkl')\n",
    "num_chunks = len(chunk_files)\n",
    "\n",
    "print(f\"\\nüìä Dataset Summary:\")\n",
    "print(f\"  Total files processed: {config['dataset_stats']['total_files']:,}\")\n",
    "print(f\"  Training samples: {config['dataset_stats']['train_size']:,} (in {num_chunks} chunks)\")\n",
    "print(f\"  Validation samples: {config['dataset_stats']['val_size']:,}\")\n",
    "print(f\"  Test samples: {config['dataset_stats']['test_size']:,}\")\n",
    "print(f\"\\nüéµ Audio Configuration:\")\n",
    "print(f\"  Sample rate: {config['target_sr']} Hz\")\n",
    "print(f\"  Duration: {config['duration']} seconds\")\n",
    "print(f\"  Mel bands: {config['n_mels']}\")\n",
    "print(f\"  MFCCs: {config['n_mfcc']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Memory-safe data ready!\")\n",
    "print(f\"üíæ Each chunk ~2GB - only 1 loaded at a time during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1133423d",
   "metadata": {},
   "source": [
    "## 5. Verify Data Generator (Optional)\n",
    "\n",
    "Quick check to ensure the generator works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ef6593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "class ChunkedDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Ultra-memory-efficient generator that loads data chunks on-demand.\n",
    "    Keeps only ONE chunk in memory at a time.\n",
    "    \"\"\"\n",
    "    def __init__(self, chunk_pattern=None, pickle_path=None, batch_size=16, \n",
    "                 feature_type='mel_spectrogram', shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.feature_type = feature_type\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        if chunk_pattern:\n",
    "            # Multi-chunk mode (for large training data)\n",
    "            self.chunk_files = sorted(glob.glob(chunk_pattern))\n",
    "            self.is_chunked = True\n",
    "            \n",
    "            # Load first chunk to get metadata\n",
    "            with open(self.chunk_files[0], 'rb') as f:\n",
    "                sample_chunk = pickle.load(f)\n",
    "            \n",
    "            self.feature_shape = sample_chunk[0]['features'][feature_type].shape\n",
    "            \n",
    "            # Calculate total samples and create index mapping\n",
    "            self.num_samples = 0\n",
    "            self.chunk_sizes = []\n",
    "            self.chunk_start_indices = [0]\n",
    "            \n",
    "            for chunk_file in self.chunk_files:\n",
    "                with open(chunk_file, 'rb') as f:\n",
    "                    chunk = pickle.load(f)\n",
    "                    size = len(chunk)\n",
    "                    self.chunk_sizes.append(size)\n",
    "                    self.num_samples += size\n",
    "                    self.chunk_start_indices.append(self.num_samples)\n",
    "                    del chunk\n",
    "                    gc.collect()\n",
    "            \n",
    "            # Current chunk in memory\n",
    "            self.current_chunk_idx = -1\n",
    "            self.current_chunk = None\n",
    "            \n",
    "            print(f\"Loaded {len(self.chunk_files)} chunks with {self.num_samples:,} total samples\")\n",
    "            \n",
    "        else:\n",
    "            # Single file mode (for val/test)\n",
    "            self.is_chunked = False\n",
    "            with open(pickle_path, 'rb') as f:\n",
    "                self.data = pickle.load(f)\n",
    "            \n",
    "            self.num_samples = len(self.data)\n",
    "            self.feature_shape = self.data[0]['features'][feature_type].shape\n",
    "            \n",
    "            print(f\"Loaded {self.num_samples:,} samples from {pickle_path}\")\n",
    "        \n",
    "        print(f\"  Feature shape: {self.feature_shape}\")\n",
    "        print(f\"  Batches per epoch: {self.__len__()} (batch_size={batch_size})\")\n",
    "        print(f\"  Using samples: {self.__len__() * batch_size}/{self.num_samples}\")\n",
    "        \n",
    "        self.indices = np.arange(self.num_samples)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def _load_chunk(self, chunk_idx):\n",
    "        \"\"\"Load a specific chunk into memory.\"\"\"\n",
    "        if chunk_idx != self.current_chunk_idx:\n",
    "            if self.current_chunk is not None:\n",
    "                del self.current_chunk\n",
    "                gc.collect()\n",
    "            \n",
    "            with open(self.chunk_files[chunk_idx], 'rb') as f:\n",
    "                self.current_chunk = pickle.load(f)\n",
    "            self.current_chunk_idx = chunk_idx\n",
    "    \n",
    "    def _get_sample(self, idx):\n",
    "        \"\"\"Get a sample by global index.\"\"\"\n",
    "        if self.is_chunked:\n",
    "            chunk_idx = 0\n",
    "            for i in range(len(self.chunk_sizes)):\n",
    "                if idx < self.chunk_start_indices[i + 1]:\n",
    "                    chunk_idx = i\n",
    "                    break\n",
    "            \n",
    "            offset = idx - self.chunk_start_indices[chunk_idx]\n",
    "            self._load_chunk(chunk_idx)\n",
    "            return self.current_chunk[offset]\n",
    "        else:\n",
    "            return self.data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches per epoch (fixed size batches only).\"\"\"\n",
    "        return self.num_samples // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Generate one batch of data with FIXED batch_size.\"\"\"\n",
    "        # All batches have exactly batch_size samples\n",
    "        start_idx = idx * self.batch_size\n",
    "        end_idx = start_idx + self.batch_size\n",
    "        \n",
    "        batch_indices = self.indices[start_idx:end_idx]\n",
    "        \n",
    "        # Pre-allocate arrays with FIXED size\n",
    "        batch_features = np.zeros(\n",
    "            (self.batch_size, *self.feature_shape, 1), \n",
    "            dtype=np.float32\n",
    "        )\n",
    "        batch_labels = np.zeros(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "        # Load batch samples\n",
    "        for i, global_idx in enumerate(batch_indices):\n",
    "            sample = self._get_sample(global_idx)\n",
    "            batch_features[i, :, :, 0] = sample['features'][self.feature_type]\n",
    "            batch_labels[i] = sample['label']['threat_level']\n",
    "        \n",
    "        return batch_features, batch_labels\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Shuffle indices and clear chunk cache after each epoch.\"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        \n",
    "        if self.is_chunked and self.current_chunk is not None:\n",
    "            del self.current_chunk\n",
    "            self.current_chunk = None\n",
    "            self.current_chunk_idx = -1\n",
    "            gc.collect()\n",
    "    \n",
    "    def get_all_labels(self):\n",
    "        \"\"\"Get all labels (memory-efficient iteration).\"\"\"\n",
    "        labels = []\n",
    "        for i in range(self.num_samples):\n",
    "            sample = self._get_sample(i)\n",
    "            labels.append(sample['label']['threat_level'])\n",
    "        return np.array(labels)\n",
    "\n",
    "# Create data generators with fixed batch size\n",
    "print(\"Creating memory-optimized data generators...\\n\")\n",
    "\n",
    "train_generator = ChunkedDataGenerator(\n",
    "    chunk_pattern='/content/data_chunks/train_chunk_*.pkl',\n",
    "    batch_size=16,\n",
    "    feature_type='mel_spectrogram',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = ChunkedDataGenerator(\n",
    "    pickle_path='/content/preprocessed_data/val_data.pkl',\n",
    "    batch_size=16,\n",
    "    feature_type='mel_spectrogram',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = ChunkedDataGenerator(\n",
    "    pickle_path='/content/preprocessed_data/test_data.pkl',\n",
    "    batch_size=16,\n",
    "    feature_type='mel_spectrogram',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Generators created with fixed batch sizes!\")\n",
    "print(f\"üíæ Memory usage: Only ~2-3GB at peak (1 chunk + model)\")\n",
    "print(f\"üìä Training batches: {len(train_generator)}, Val: {len(val_generator)}, Test: {len(test_generator)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de955d",
   "metadata": {},
   "source": [
    "## 4. Create Memory-Efficient Data Generator\n",
    "\n",
    "**Important:** The preprocessed data is 20GB, but Colab free tier has only ~12GB RAM. We'll use a data generator to load batches on-demand instead of loading everything at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1bed08",
   "metadata": {},
   "source": [
    "## 5. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e8ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generator by loading one batch\n",
    "print(\"Testing chunked data generator...\")\n",
    "X_batch, y_batch = train_generator[0]\n",
    "\n",
    "print(f\"\\n‚úÖ Batch loaded successfully!\")\n",
    "print(f\"  Features: {X_batch.shape}\")\n",
    "print(f\"  Labels: {y_batch.shape}\")\n",
    "\n",
    "print(f\"\\nLabel distribution in first batch:\")\n",
    "class_names = ['BACKGROUND', 'THREAT_CONTEXT', 'THREAT']\n",
    "for i, name in enumerate(class_names):\n",
    "    count = np.sum(y_batch == i)\n",
    "    print(f\"  {name}: {count} samples\")\n",
    "\n",
    "print(f\"\\nFeature statistics:\")\n",
    "print(f\"  Min: {X_batch.min():.4f}\")\n",
    "print(f\"  Max: {X_batch.max():.4f}\")\n",
    "print(f\"  Mean: {X_batch.mean():.4f}\")\n",
    "\n",
    "# Check memory usage\n",
    "import psutil\n",
    "import os\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "print(f\"\\nüíæ Current memory usage: {memory_mb:.1f} MB\")\n",
    "\n",
    "print(\"\\n‚úÖ Generator working correctly with minimal memory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f0db1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "# Load ONE sample from training data for visualization (safe memory-wise)\n",
    "print(\"Loading sample for visualization...\")\n",
    "X_sample, y_sample = train_generator[0]\n",
    "\n",
    "# Get the first sample from the batch\n",
    "sample_idx = 0\n",
    "features = X_sample[sample_idx, :, :, 0]  # Remove channel dimension for display\n",
    "label = y_sample[sample_idx]\n",
    "\n",
    "# Access sample metadata from the generator's current chunk\n",
    "if train_generator.is_chunked:\n",
    "    # Load first chunk temporarily for metadata\n",
    "    train_generator._load_chunk(0)\n",
    "    sample_data = train_generator.current_chunk[sample_idx]\n",
    "else:\n",
    "    sample_data = train_generator.data[sample_idx]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Mel-spectrogram\n",
    "img1 = librosa.display.specshow(\n",
    "    features,\n",
    "    sr=config['target_sr'],\n",
    "    hop_length=config['hop_length'],\n",
    "    x_axis='time',\n",
    "    y_axis='mel',\n",
    "    ax=axes[0],\n",
    "    cmap='viridis'\n",
    ")\n",
    "axes[0].set_title(\n",
    "    f\"Sample: {sample_data['label']['threat_level_name']} - {sample_data['label']['subcategory']}\\nMel-Spectrogram (128 bands)\",\n",
    "    fontweight='bold',\n",
    "    fontsize=12\n",
    ")\n",
    "fig.colorbar(img1, ax=axes[0], format='%+2.0f dB')\n",
    "\n",
    "# MFCCs\n",
    "mfcc = sample_data['features']['mfcc']\n",
    "img2 = librosa.display.specshow(\n",
    "    mfcc,\n",
    "    sr=config['target_sr'],\n",
    "    hop_length=config['hop_length'],\n",
    "    x_axis='time',\n",
    "    ax=axes[1],\n",
    "    cmap='coolwarm'\n",
    ")\n",
    "axes[1].set_title('MFCCs (40 coefficients)', fontweight='bold')\n",
    "axes[1].set_ylabel('MFCC Coefficient')\n",
    "fig.colorbar(img2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFile: {sample_data['label']['file_name']}\")\n",
    "print(f\"Threat Level: {class_names[label]} (class {label})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b6b58",
   "metadata": {},
   "source": [
    "## 6. Load Model Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a8897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for model training\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Enable GPU memory growth\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"‚úÖ GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6c3e09",
   "metadata": {},
   "source": [
    "## 7. Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ab2684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Get labels efficiently (loads chunks one at a time)\n",
    "print(\"Computing class weights from training data...\")\n",
    "print(\"‚è≥ This may take a moment as it scans all chunks...\\n\")\n",
    "\n",
    "y_train_all = train_generator.get_all_labels()\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_all),\n",
    "    y=y_train_all\n",
    ")\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "print(\"Class weights (for balanced training):\")\n",
    "class_names = ['BACKGROUND', 'THREAT_CONTEXT', 'THREAT']\n",
    "for cls, weight in class_weight_dict.items():\n",
    "    count = np.sum(y_train_all == cls)\n",
    "    print(f\"  {class_names[cls]}: {weight:.3f} (n={count:,})\")\n",
    "\n",
    "# Clean up labels to free memory\n",
    "del y_train_all\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nInput shape for model: {train_generator.feature_shape + (1,)}\")\n",
    "print(\"‚úÖ Ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae6178",
   "metadata": {},
   "source": [
    "## 8. Build CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb2d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(input_shape, num_classes=3):\n",
    "    \"\"\"\n",
    "    Build CNN model for threat detection.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Input\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # Conv Block 1\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Conv Block 2\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Conv Block 3\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Conv Block 4\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Global pooling and dense\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Output\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model with correct input shape\n",
    "input_shape = train_generator.feature_shape + (1,)  # Add channel dimension\n",
    "model = build_cnn_model(input_shape=input_shape)\n",
    "model.summary()\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Model compiled successfully!\")\n",
    "print(f\"Input shape: {input_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a75c9b",
   "metadata": {},
   "source": [
    "## 9. Setup Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e1b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model directory\n",
    "!mkdir -p /content/models\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = callbacks.ModelCheckpoint(\n",
    "    filepath='/content/models/best_model.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "tensorboard = callbacks.TensorBoard(\n",
    "    log_dir='/content/models/logs',\n",
    "    histogram_freq=1\n",
    ")\n",
    "\n",
    "callback_list = [early_stopping, model_checkpoint, reduce_lr, tensorboard]\n",
    "\n",
    "print(\"‚úÖ Callbacks configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ccc2c9",
   "metadata": {},
   "source": [
    "## 10. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ced5dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting memory-optimized training...\\n\")\n",
    "print(\"üí° Batch size: 16 (optimized for 12GB RAM)\")\n",
    "print(\"üíæ Only 1 data chunk loaded at a time (~2GB)\")\n",
    "print(\"üîÑ Chunks rotate automatically during training\\n\")\n",
    "\n",
    "# Configure GPU\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "print(f\"üìä Dataset info:\")\n",
    "print(f\"  Training batches: {len(train_generator)} √ó {train_generator.batch_size} samples\")\n",
    "print(f\"  Validation batches: {len(val_generator)} √ó {val_generator.batch_size} samples\")\n",
    "print(f\"  Total training samples: {len(train_generator) * train_generator.batch_size:,}\")\n",
    "print(f\"  Total validation samples: {len(val_generator) * val_generator.batch_size:,}\")\n",
    "print(f\"  All batches have FIXED size (incomplete batches dropped)\\n\")\n",
    "\n",
    "print(\"üí° Training optimizations:\")\n",
    "print(\"  ‚úì Chunked loading: Only 1 chunk (~2GB) in memory at a time\")\n",
    "print(\"  ‚úì GPU memory growth: Dynamic allocation prevents OOM\")\n",
    "print(\"  ‚úì Early stopping: Prevents overfitting (patience=15)\")\n",
    "print(\"  ‚úì Learning rate reduction: Adapts when validation loss plateaus\")\n",
    "print(\"  ‚úì Dropout & BatchNorm: Built-in regularization\\n\")\n",
    "\n",
    "print(\"‚ö†Ô∏è  Note: Class weighting disabled due to TensorFlow compatibility issues\")\n",
    "print(\"   Model will learn from natural class distribution in augmented dataset\\n\")\n",
    "\n",
    "# Train with generators\n",
    "print(\"‚è≥ Training will take approximately 20-40 minutes with GPU...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=100,\n",
    "    callbacks=callback_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde46151",
   "metadata": {},
   "source": [
    "## 11. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420a60f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\\n\")\n",
    "test_results = model.evaluate(test_generator, verbose=1)\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"  Loss: {test_results[0]:.4f}\")\n",
    "print(f\"  Accuracy: {test_results[1]:.4f}\")\n",
    "print(f\"  Precision: {test_results[2]:.4f}\")\n",
    "print(f\"  Recall: {test_results[3]:.4f}\")\n",
    "\n",
    "# Get predictions (batch by batch to save memory)\n",
    "print(\"\\nGenerating predictions...\")\n",
    "y_pred_proba = model.predict(test_generator, verbose=1)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Get true labels\n",
    "y_test = test_generator.get_all_labels()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted', fontweight='bold')\n",
    "plt.ylabel('True', fontweight='bold')\n",
    "plt.title('Confusion Matrix - Test Set', fontweight='bold', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee85705b",
   "metadata": {},
   "source": [
    "## 12. Save Model and Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "model.save('/content/models/threat_detection_final.keras')\n",
    "print(\"‚úÖ Model saved locally\")\n",
    "\n",
    "# Export to TensorFlow Lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('/content/models/threat_detection.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"‚úÖ TensorFlow Lite model: {len(tflite_model) / 1024:.1f} KB\")\n",
    "\n",
    "# Save model configuration\n",
    "input_shape = train_generator.feature_shape + (1,)\n",
    "model_config = {\n",
    "    'test_accuracy': float(test_results[1]),\n",
    "    'test_precision': float(test_results[2]),\n",
    "    'test_recall': float(test_results[3]),\n",
    "    'class_names': class_names,\n",
    "    'input_shape': list(input_shape),\n",
    "    'preprocessing': config\n",
    "}\n",
    "\n",
    "with open('/content/models/model_config.json', 'w') as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Model configuration saved\")\n",
    "\n",
    "# Upload to S3\n",
    "print(\"\\nUploading model to S3...\")\n",
    "!aws s3 cp /content/models/ s3://{S3_BUCKET}/models/ --recursive\n",
    "\n",
    "print(\"\\n‚úÖ Model uploaded to S3!\")\n",
    "print(f\"   Location: s3://{S3_BUCKET}/models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078767cb",
   "metadata": {},
   "source": [
    "## 13. Download Model to Local Machine (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f738567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Download trained model\n",
    "files.download('/content/models/threat_detection_final.keras')\n",
    "files.download('/content/models/threat_detection.tflite')\n",
    "files.download('/content/models/model_config.json')\n",
    "\n",
    "print(\"‚úÖ Files queued for download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bb3cd6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What This Notebook Does:\n",
    "\n",
    "1. ‚úÖ **Setup**: Installs libraries and configures AWS access\n",
    "2. ‚úÖ **Data Download**: Pulls 6,734 audio files from S3 bucket\n",
    "3. ‚úÖ **Preprocessing**: Processes audio with environmental augmentation\n",
    "4. ‚úÖ **Feature Extraction**: Mel-spectrograms (128 bands) for CNN\n",
    "5. ‚úÖ **Model Training**: Deep CNN with class balancing\n",
    "6. ‚úÖ **Evaluation**: Precision, recall, F1-score per threat level\n",
    "7. ‚úÖ **Export**: Keras model + TensorFlow Lite (edge deployment)\n",
    "8. ‚úÖ **Upload**: Saves trained model back to S3\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **Environmental Augmentation**: Mixes threat sounds with wind/rain\n",
    "- **Three-Tier Classification**: THREAT (2), THREAT_CONTEXT (1), BACKGROUND (0)\n",
    "- **Class Balancing**: Weighted loss for imbalanced dataset\n",
    "- **GPU Acceleration**: Faster training on Colab's GPU\n",
    "- **Production Ready**: TFLite model for Raspberry Pi deployment\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Deploy TFLite model to edge devices\n",
    "2. Integrate with ranger alert system\n",
    "3. Monitor model performance in field\n",
    "4. Collect feedback for model improvement"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
