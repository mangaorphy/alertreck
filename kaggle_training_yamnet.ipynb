{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6373e9",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef532550",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q librosa soundfile awscli boto3 tensorflow-hub\n",
    "\n",
    "print(\"‚úÖ All libraries installed successfully!\")\n",
    "print(\"üì¶ TensorFlow Hub installed for YAMNet model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71fc93",
   "metadata": {},
   "source": [
    "## 2. Configure AWS S3 Access\n",
    "\n",
    "**Add secrets in Kaggle:**\n",
    "1. Settings ‚Üí Add-ons ‚Üí Secrets\n",
    "2. Add: `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e678da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Load AWS credentials from Kaggle Secrets\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "try:\n",
    "    os.environ['AWS_ACCESS_KEY_ID'] = user_secrets.get_secret('AWS_ACCESS_KEY_ID')\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY'] = user_secrets.get_secret('AWS_SECRET_ACCESS_KEY')\n",
    "    os.environ['AWS_DEFAULT_REGION'] = user_secrets.get_secret('AWS_REGION')\n",
    "    print(\"‚úÖ AWS credentials loaded from Kaggle secrets\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  Kaggle secrets not found. Add them in Settings ‚Üí Secrets\")\n",
    "    raise\n",
    "\n",
    "# Verify AWS access\n",
    "!aws s3 ls s3://alertreck/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8502f47d",
   "metadata": {},
   "source": [
    "## 3. Download Preprocessed Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df96d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create working directory\n",
    "!mkdir -p /kaggle/working/preprocessed_data\n",
    "!mkdir -p /kaggle/working/train_chunks\n",
    "\n",
    "S3_BUCKET = \"alertreck\"\n",
    "DATA_DIR = \"/kaggle/working/preprocessed_data\"\n",
    "\n",
    "print(\"üì• Downloading preprocessed data from S3...\")\n",
    "print(\"Files: train_chunks (10x ~2GB), val_data.pkl (960MB)\")\n",
    "print(\"‚è∞ This may take 10-15 minutes depending on connection speed.\\n\")\n",
    "\n",
    "# Download chunked training data\n",
    "print(\"Downloading training chunks...\")\n",
    "!aws s3 sync s3://{S3_BUCKET}/preprocessed_data/train_chunks/ /kaggle/working/train_chunks/\n",
    "\n",
    "# Download validation data and config\n",
    "print(\"\\nDownloading validation data...\")\n",
    "!aws s3 cp s3://{S3_BUCKET}/preprocessed_data/val_data.pkl {DATA_DIR}/val_data.pkl\n",
    "!aws s3 cp s3://{S3_BUCKET}/preprocessed_data/preprocessing_config.json {DATA_DIR}/preprocessing_config.json\n",
    "\n",
    "print(\"\\n‚úÖ All data downloaded!\")\n",
    "\n",
    "# Load configuration\n",
    "import json\n",
    "with open(f'{DATA_DIR}/preprocessing_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(f\"\\nüìä Dataset Summary:\")\n",
    "print(f\"  Training samples: {config['dataset_stats']['train_size']:,}\")\n",
    "print(f\"  Validation samples: {config['dataset_stats']['val_size']:,}\")\n",
    "print(f\"  Sample rate: {config['target_sr']} Hz\")\n",
    "print(f\"  Duration: {config['duration']} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c73b6c",
   "metadata": {},
   "source": [
    "## 4. Load YAMNet Pretrained Model\n",
    "\n",
    "YAMNet is a deep neural network trained on AudioSet to predict audio events. We'll use it as a feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c15308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "\n",
    "print(\"üîß Loading YAMNet model from TensorFlow Hub...\")\n",
    "print(\"This may take a few minutes on first run...\\n\")\n",
    "\n",
    "# Load YAMNet model\n",
    "YAMNET_MODEL_URL = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(YAMNET_MODEL_URL)\n",
    "\n",
    "print(\"‚úÖ YAMNet model loaded successfully!\")\n",
    "print(\"\\nüìã YAMNet Details:\")\n",
    "print(\"  - Pre-trained on AudioSet (2M+ audio clips, 521 classes)\")\n",
    "print(\"  - Input: 16 kHz mono audio waveform\")\n",
    "print(\"  - Output: 1024-dimensional embedding per 0.96s frame\")\n",
    "print(\"  - Architecture: MobileNetV1 (efficient for audio)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4739cd",
   "metadata": {},
   "source": [
    "## 5. Load Preprocessed Data and Reconstruct Audio\n",
    "\n",
    "We need to convert mel-spectrograms back to audio waveforms for YAMNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56cd0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import librosa\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"üìÇ Loading preprocessed data...\\n\")\n",
    "\n",
    "# Load training data from chunks\n",
    "print(\"Loading training chunks...\")\n",
    "import glob\n",
    "train_data = []\n",
    "chunk_files = sorted(glob.glob('/kaggle/working/train_chunks/train_chunk_*.pkl'))\n",
    "for chunk_file in chunk_files:\n",
    "    with open(chunk_file, 'rb') as f:\n",
    "        chunk = pickle.load(f)\n",
    "        train_data.extend(chunk)\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(train_data):,} training samples\")\n",
    "\n",
    "# Load validation data\n",
    "with open(f'{DATA_DIR}/val_data.pkl', 'rb') as f:\n",
    "    val_data = pickle.load(f)\n",
    "print(f\"‚úÖ Loaded {len(val_data):,} validation samples\")\n",
    "\n",
    "print(f\"\\nüìä Total samples: {len(train_data) + len(val_data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0372ac6",
   "metadata": {},
   "source": [
    "## 6. Extract YAMNet Embeddings from Audio\n",
    "\n",
    "Convert mel-spectrograms to audio and extract YAMNet features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44641a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_to_audio(mel_spec_db, sr=22050, n_fft=2048, hop_length=512):\n",
    "    \"\"\"\n",
    "    Convert mel-spectrogram (dB) back to audio waveform using Griffin-Lim.\n",
    "    \n",
    "    Args:\n",
    "        mel_spec_db: Mel-spectrogram in dB (128, 431)\n",
    "        sr: Sample rate\n",
    "        n_fft: FFT window size\n",
    "        hop_length: Hop length\n",
    "        \n",
    "    Returns:\n",
    "        Audio waveform\n",
    "    \"\"\"\n",
    "    # Convert from dB to power\n",
    "    mel_spec = librosa.db_to_power(mel_spec_db)\n",
    "    \n",
    "    # Inverse mel-spectrogram to linear spectrogram\n",
    "    spec = librosa.feature.inverse.mel_to_stft(mel_spec, sr=sr, n_fft=n_fft)\n",
    "    \n",
    "    # Reconstruct audio using Griffin-Lim algorithm\n",
    "    audio = librosa.griffinlim(spec, hop_length=hop_length, n_iter=32)\n",
    "    \n",
    "    return audio\n",
    "\n",
    "\n",
    "def resample_audio(audio, orig_sr, target_sr=16000):\n",
    "    \"\"\"\n",
    "    Resample audio to target sample rate (YAMNet expects 16 kHz).\n",
    "    \n",
    "    Args:\n",
    "        audio: Input audio waveform\n",
    "        orig_sr: Original sample rate\n",
    "        target_sr: Target sample rate (16 kHz for YAMNet)\n",
    "        \n",
    "    Returns:\n",
    "        Resampled audio\n",
    "    \"\"\"\n",
    "    if orig_sr != target_sr:\n",
    "        audio = librosa.resample(audio, orig_sr=orig_sr, target_sr=target_sr)\n",
    "    return audio\n",
    "\n",
    "\n",
    "def extract_yamnet_embedding(audio_waveform):\n",
    "    \"\"\"\n",
    "    Extract YAMNet embedding from audio waveform.\n",
    "    \n",
    "    Args:\n",
    "        audio_waveform: Audio waveform at 16 kHz\n",
    "        \n",
    "    Returns:\n",
    "        Mean YAMNet embedding (1024-dimensional vector)\n",
    "    \"\"\"\n",
    "    # YAMNet expects float32 tensor\n",
    "    audio_tensor = tf.convert_to_tensor(audio_waveform, dtype=tf.float32)\n",
    "    \n",
    "    # Extract embeddings (scores, embeddings, spectrogram)\n",
    "    scores, embeddings, spectrogram = yamnet_model(audio_tensor)\n",
    "    \n",
    "    # Average embeddings across time frames (10s audio ‚Üí multiple frames)\n",
    "    # Each frame is 0.96s, so 10s audio ‚Üí ~10 frames\n",
    "    mean_embedding = tf.reduce_mean(embeddings, axis=0).numpy()\n",
    "    \n",
    "    return mean_embedding\n",
    "\n",
    "\n",
    "print(\"‚úÖ YAMNet feature extraction functions ready\")\n",
    "print(\"\\nüìã Processing pipeline:\")\n",
    "print(\"  1. Mel-spectrogram (128, 431) ‚Üí Audio waveform (22050 Hz)\")\n",
    "print(\"  2. Resample audio (22050 Hz ‚Üí 16000 Hz for YAMNet)\")\n",
    "print(\"  3. Extract YAMNet embeddings (1024 features)\")\n",
    "print(\"  4. Average embeddings across time frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d179a",
   "metadata": {},
   "source": [
    "## 7. Process All Audio Files with YAMNet\n",
    "\n",
    "Extract YAMNet embeddings for all training and validation samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98e088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_with_yamnet(data, split_name=\"train\"):\n",
    "    \"\"\"\n",
    "    Process all samples in dataset and extract YAMNet embeddings.\n",
    "    \n",
    "    Args:\n",
    "        data: List of preprocessed samples (mel-spectrograms + labels)\n",
    "        split_name: Name of split (for progress display)\n",
    "        \n",
    "    Returns:\n",
    "        X: YAMNet embeddings (N, 1024)\n",
    "        y: Labels (N,)\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    \n",
    "    print(f\"\\nüîÑ Processing {split_name} data with YAMNet...\")\n",
    "    print(f\"Total samples: {len(data):,}\")\n",
    "    print(\"This will take 5-15 minutes depending on dataset size...\\n\")\n",
    "    \n",
    "    for sample in tqdm(data, desc=f\"Extracting {split_name} embeddings\"):\n",
    "        try:\n",
    "            # Get mel-spectrogram\n",
    "            mel_spec = sample['features']['mel_spectrogram']\n",
    "            \n",
    "            # Convert mel-spectrogram to audio\n",
    "            audio = mel_to_audio(\n",
    "                mel_spec,\n",
    "                sr=config['target_sr'],\n",
    "                n_fft=config['n_fft'],\n",
    "                hop_length=config['hop_length']\n",
    "            )\n",
    "            \n",
    "            # Resample to 16 kHz (YAMNet requirement)\n",
    "            audio_16k = resample_audio(audio, orig_sr=config['target_sr'], target_sr=16000)\n",
    "            \n",
    "            # Extract YAMNet embedding\n",
    "            embedding = extract_yamnet_embedding(audio_16k)\n",
    "            \n",
    "            # Store results\n",
    "            embeddings.append(embedding)\n",
    "            labels.append(sample['label']['threat_level'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing sample: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(embeddings, dtype=np.float32)\n",
    "    y = np.array(labels, dtype=np.int32)\n",
    "    \n",
    "    print(f\"\\n‚úÖ {split_name} processing complete!\")\n",
    "    print(f\"  Embeddings shape: {X.shape}\")\n",
    "    print(f\"  Labels shape: {y.shape}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Process training data\n",
    "X_train, y_train = process_dataset_with_yamnet(train_data, \"train\")\n",
    "\n",
    "# Free memory\n",
    "del train_data\n",
    "gc.collect()\n",
    "\n",
    "# Process validation data\n",
    "X_val, y_val = process_dataset_with_yamnet(val_data, \"validation\")\n",
    "\n",
    "# Free memory\n",
    "del val_data\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nüìä Final Dataset Shapes:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  X_val: {X_val.shape}\")\n",
    "print(f\"  y_val: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e526fe96",
   "metadata": {},
   "source": [
    "## 8. Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cea52a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_names = ['BACKGROUND', 'THREAT_CONTEXT', 'THREAT']\n",
    "\n",
    "print(\"Computing class weights from training data...\")\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "print(\"\\nClass weights (for balanced training):\")\n",
    "for cls, weight in class_weight_dict.items():\n",
    "    count = np.sum(y_train == cls)\n",
    "    print(f\"  {class_names[cls]}: {weight:.3f} (n={count:,})\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data ready for training!\")\n",
    "print(f\"  Input: YAMNet embeddings (1024 features)\")\n",
    "print(f\"  Output: 3 threat classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884719f1",
   "metadata": {},
   "source": [
    "## 9. Build Dense Model for YAMNet Embeddings\n",
    "\n",
    "Since YAMNet embeddings are 1D vectors (not 2D images), we use a Dense Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7057fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Configure GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"‚úÖ GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not set memory growth: {e}\")\n",
    "\n",
    "# Enable mixed precision\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "print(\"‚úÖ Mixed precision enabled\\n\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"üöÄ Building Dense Neural Network for YAMNet Embeddings...\\n\")\n",
    "\n",
    "def build_yamnet_classifier(input_dim=1024, num_classes=3):\n",
    "    \"\"\"\n",
    "    Build dense classifier for YAMNet embeddings.\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Dimension of YAMNet embeddings (1024)\n",
    "        num_classes: Number of output classes (3)\n",
    "        \n",
    "    Returns:\n",
    "        Keras model\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        \n",
    "        # Dense block 1\n",
    "        layers.Dense(512, activation='relu',\n",
    "                    kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # Dense block 2\n",
    "        layers.Dense(256, activation='relu',\n",
    "                    kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        # Dense block 3\n",
    "        layers.Dense(128, activation='relu',\n",
    "                    kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(num_classes, activation='softmax', dtype='float32')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Build model\n",
    "model = build_yamnet_classifier(input_dim=1024, num_classes=3)\n",
    "model.summary()\n",
    "\n",
    "print(f\"\\nüìä Model parameters: {model.count_params():,}\")\n",
    "print(\"üí° Simple dense network on top of YAMNet features\")\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Model compiled!\")\n",
    "print(\"   Architecture: 3-layer dense network (512‚Üí256‚Üí128‚Üí3)\")\n",
    "print(\"   Regularization: L2 + BatchNorm + Dropout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ff284b",
   "metadata": {},
   "source": [
    "## 10. Setup Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af9534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "\n",
    "# Create model directory\n",
    "!mkdir -p /kaggle/working/models\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = callbacks.ModelCheckpoint(\n",
    "    filepath='/kaggle/working/models/best_yamnet_classifier.weights.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callback_list = [early_stopping, model_checkpoint, reduce_lr]\n",
    "\n",
    "print(\"‚úÖ Callbacks configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944a2c95",
   "metadata": {},
   "source": [
    "## 11. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b5b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting YAMNet Classifier Training...\\n\")\n",
    "print(\"üí° Training dense network on YAMNet embeddings\")\n",
    "print(\"üéØ Class weighting enabled for balanced training\")\n",
    "print(\"‚ö° Mixed precision + GPU acceleration\\n\")\n",
    "\n",
    "print(f\"üìä Dataset info:\")\n",
    "print(f\"  Training samples: {len(X_train):,}\")\n",
    "print(f\"  Validation samples: {len(X_val):,}\")\n",
    "print(f\"  Feature dimension: {X_train.shape[1]}\\n\")\n",
    "\n",
    "print(\"‚è≥ Expected training time: 2-5 minutes with GPU...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callback_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb2b53c",
   "metadata": {},
   "source": [
    "## 12. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae017c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Train')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation')\n",
    "axes[0].set_title('Model Loss (YAMNet Classifier)', fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history.history['accuracy'], label='Train')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation')\n",
    "axes[1].set_title('Model Accuracy (YAMNet Classifier)', fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d926727",
   "metadata": {},
   "source": [
    "## 13. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6bdfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"üìä Evaluating on validation set...\\n\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_results = model.evaluate(X_val, y_val, verbose=1)\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "print(f\"  Loss: {val_results[0]:.4f}\")\n",
    "print(f\"  Accuracy: {val_results[1]:.4f}\")\n",
    "\n",
    "# Get predictions\n",
    "print(\"\\nGenerating predictions...\")\n",
    "y_pred_proba = model.predict(X_val, verbose=1)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=class_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted', fontweight='bold')\n",
    "plt.ylabel('True', fontweight='bold')\n",
    "plt.title('Confusion Matrix - YAMNet Classifier', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\")\n",
    "test_results = val_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463dd47c",
   "metadata": {},
   "source": [
    "## 14. Save Model and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e40b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full model\n",
    "model.save('/kaggle/working/models/yamnet_classifier.keras')\n",
    "print(\"‚úÖ Full model saved\")\n",
    "\n",
    "# Load best weights\n",
    "model.load_weights('/kaggle/working/models/best_yamnet_classifier.weights.h5')\n",
    "print(\"‚úÖ Loaded best weights from checkpoint\")\n",
    "\n",
    "# Export to TensorFlow Lite\n",
    "print(\"\\nExporting to TensorFlow Lite...\")\n",
    "print(\"Converting mixed precision model to float32...\")\n",
    "\n",
    "# Create float32 model\n",
    "tf.keras.mixed_precision.set_global_policy('float32')\n",
    "model_f32 = build_yamnet_classifier(input_dim=1024, num_classes=3)\n",
    "model_f32.set_weights(model.get_weights())\n",
    "print(\"‚úÖ Created float32 model\")\n",
    "\n",
    "# Convert to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_f32)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('/kaggle/working/models/yamnet_classifier.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"‚úÖ TensorFlow Lite model: {len(tflite_model) / 1024:.1f} KB\")\n",
    "\n",
    "# Save model configuration\n",
    "model_config = {\n",
    "    'model_type': 'YAMNet_Classifier',\n",
    "    'feature_extractor': 'YAMNet (TensorFlow Hub)',\n",
    "    'embedding_dim': 1024,\n",
    "    'val_accuracy': float(test_results[1]) if test_results else None,\n",
    "    'val_loss': float(test_results[0]) if test_results else None,\n",
    "    'class_names': class_names,\n",
    "    'preprocessing': config,\n",
    "    'total_parameters': int(model.count_params())\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('/kaggle/working/models/yamnet_classifier_config.json', 'w') as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Model configuration saved\")\n",
    "\n",
    "# Upload to S3\n",
    "print(\"\\nUploading models to S3...\")\n",
    "!aws s3 cp /kaggle/working/models/yamnet_classifier.keras s3://{S3_BUCKET}/models/yamnet/\n",
    "!aws s3 cp /kaggle/working/models/best_yamnet_classifier.weights.h5 s3://{S3_BUCKET}/models/yamnet/\n",
    "!aws s3 cp /kaggle/working/models/yamnet_classifier.tflite s3://{S3_BUCKET}/models/yamnet/\n",
    "!aws s3 cp /kaggle/working/models/yamnet_classifier_config.json s3://{S3_BUCKET}/models/yamnet/\n",
    "\n",
    "print(\"\\n‚úÖ Models uploaded to S3!\")\n",
    "print(f\"   Location: s3://{S3_BUCKET}/models/yamnet/\")\n",
    "print(\"\\nüì¶ Files uploaded:\")\n",
    "print(\"  - yamnet_classifier.keras (full model)\")\n",
    "print(\"  - best_yamnet_classifier.weights.h5 (best weights)\")\n",
    "print(\"  - yamnet_classifier.tflite (edge deployment)\")\n",
    "print(\"  - yamnet_classifier_config.json (configuration)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d1e725",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### YAMNet Transfer Learning Complete! üéâ\n",
    "\n",
    "**Approach:**\n",
    "- ‚úÖ Used YAMNet (pre-trained on AudioSet) as feature extractor\n",
    "- ‚úÖ Extracted 1024-dimensional embeddings from audio\n",
    "- ‚úÖ Trained simple dense classifier on embeddings\n",
    "- ‚úÖ Much faster training than training CNN from scratch\n",
    "\n",
    "**Model Architecture:**\n",
    "1. **Feature Extraction**: YAMNet (frozen, pre-trained)\n",
    "2. **Classifier**: 3-layer dense network (512‚Üí256‚Üí128‚Üí3)\n",
    "3. **Regularization**: L2, BatchNorm, Dropout\n",
    "\n",
    "**Advantages:**\n",
    "- üöÄ Fast training (2-5 minutes)\n",
    "- üí° Leverages AudioSet knowledge (521 audio classes)\n",
    "- üì¶ Small model size (classifier only)\n",
    "- üéØ Strong generalization from pre-trained features\n",
    "\n",
    "**Deployment:**\n",
    "- For Raspberry Pi: Use YAMNet TFLite + classifier TFLite\n",
    "- Two-stage inference: YAMNet embeddings ‚Üí classifier\n",
    "\n",
    "**Next Steps:**\n",
    "1. Compare with Custom CNN results\n",
    "2. Deploy best model to Raspberry Pi\n",
    "3. Integrate with ranger alert system"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
