{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c926b72",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fc1fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Audio processing\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Machine learning utilities\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, \n",
    "    precision_recall_fscore_support, roc_auc_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56a5428",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ff7c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory\n",
    "DATA_DIR = Path(\"/Users/cococe/Desktop/AUDIOSET METADATA/preprocessed_data\")\n",
    "\n",
    "# Load configuration\n",
    "with open(DATA_DIR / 'preprocessing_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"Preprocessing Configuration:\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f400b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data splits\n",
    "def load_split(split_name: str) -> List[Dict]:\n",
    "    \"\"\"Load a data split.\"\"\"\n",
    "    with open(DATA_DIR / f'{split_name}_data.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "print(\"Loading data splits...\")\n",
    "train_data = load_split('train')\n",
    "val_data = load_split('val')\n",
    "test_data = load_split('test')\n",
    "\n",
    "print(f\"\\nTrain samples: {len(train_data):,}\")\n",
    "print(f\"Validation samples: {len(val_data):,}\")\n",
    "print(f\"Test samples: {len(test_data):,}\")\n",
    "print(f\"Total: {len(train_data) + len(val_data) + len(test_data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b7006",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4241b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze threat level distribution\n",
    "def get_label_distribution(data: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"Get label distribution statistics.\"\"\"\n",
    "    threat_levels = [sample['label']['threat_level'] for sample in data]\n",
    "    threat_names = [sample['label']['threat_level_name'] for sample in data]\n",
    "    subcategories = [sample['label']['subcategory'] for sample in data]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'threat_level': threat_levels,\n",
    "        'threat_name': threat_names,\n",
    "        'subcategory': subcategories\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = get_label_distribution(train_data)\n",
    "val_df = get_label_distribution(val_data)\n",
    "test_df = get_label_distribution(test_data)\n",
    "\n",
    "print(\"\\n=== TRAIN SET ===\")\n",
    "print(train_df['threat_name'].value_counts())\n",
    "print(\"\\n=== VALIDATION SET ===\")\n",
    "print(val_df['threat_name'].value_counts())\n",
    "print(\"\\n=== TEST SET ===\")\n",
    "print(test_df['threat_name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cd9720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Threat level distribution\n",
    "threat_counts = train_df['threat_name'].value_counts()\n",
    "axes[0, 0].bar(threat_counts.index, threat_counts.values, color=['red', 'orange', 'green'])\n",
    "axes[0, 0].set_title('Training Set - Threat Level Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Number of Samples')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Subcategory distribution\n",
    "subcat_counts = train_df['subcategory'].value_counts()\n",
    "axes[0, 1].barh(subcat_counts.index, subcat_counts.values)\n",
    "axes[0, 1].set_title('Training Set - Subcategory Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Number of Samples')\n",
    "\n",
    "# Threat level percentages\n",
    "threat_pcts = train_df['threat_name'].value_counts(normalize=True) * 100\n",
    "axes[1, 0].pie(threat_pcts.values, labels=threat_pcts.index, autopct='%1.1f%%',\n",
    "               colors=['red', 'orange', 'green'], startangle=90)\n",
    "axes[1, 0].set_title('Threat Level Percentage', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Train/Val/Test split sizes\n",
    "split_sizes = [len(train_data), len(val_data), len(test_data)]\n",
    "split_names = ['Train', 'Validation', 'Test']\n",
    "axes[1, 1].bar(split_names, split_sizes, color=['blue', 'orange', 'green'])\n",
    "axes[1, 1].set_title('Dataset Split Sizes', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Number of Samples')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad37e822",
   "metadata": {},
   "source": [
    "## 4. Visualize Audio Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac50867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find one sample from each threat level\n",
    "threat_samples = {}\n",
    "for idx, sample in enumerate(train_data):\n",
    "    threat_name = sample['label']['threat_level_name']\n",
    "    if threat_name not in threat_samples:\n",
    "        threat_samples[threat_name] = sample\n",
    "    if len(threat_samples) == 3:\n",
    "        break\n",
    "\n",
    "# Visualize each threat level\n",
    "for threat_name, sample in threat_samples.items():\n",
    "    features = sample['features']\n",
    "    label = sample['label']\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Waveform\n",
    "    waveform = features['waveform']\n",
    "    time = np.arange(len(waveform)) / config['target_sr']\n",
    "    axes[0].plot(time, waveform, linewidth=0.5)\n",
    "    axes[0].set_xlabel('Time (s)')\n",
    "    axes[0].set_ylabel('Amplitude')\n",
    "    axes[0].set_title(f\"{threat_name} - {label['subcategory']} - Waveform\", fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mel-spectrogram\n",
    "    mel_spec = features['mel_spectrogram']\n",
    "    img1 = librosa.display.specshow(\n",
    "        mel_spec,\n",
    "        sr=config['target_sr'],\n",
    "        hop_length=config['hop_length'],\n",
    "        x_axis='time',\n",
    "        y_axis='mel',\n",
    "        ax=axes[1],\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    axes[1].set_title('Mel-Spectrogram (128 bands)', fontweight='bold')\n",
    "    fig.colorbar(img1, ax=axes[1], format='%+2.0f dB')\n",
    "    \n",
    "    # MFCCs\n",
    "    mfcc = features['mfcc']\n",
    "    img2 = librosa.display.specshow(\n",
    "        mfcc,\n",
    "        sr=config['target_sr'],\n",
    "        hop_length=config['hop_length'],\n",
    "        x_axis='time',\n",
    "        ax=axes[2],\n",
    "        cmap='coolwarm'\n",
    "    )\n",
    "    axes[2].set_title('MFCCs (40 coefficients)', fontweight='bold')\n",
    "    axes[2].set_ylabel('MFCC Coefficient')\n",
    "    fig.colorbar(img2, ax=axes[2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc6019",
   "metadata": {},
   "source": [
    "## 5. Prepare Data for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a56d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data: List[Dict], feature_type: str = 'mel_spectrogram') -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Prepare features and labels for training.\n",
    "    \n",
    "    Args:\n",
    "        data: List of samples\n",
    "        feature_type: Type of features to extract\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (features, labels)\n",
    "    \"\"\"\n",
    "    features = np.array([sample['features'][feature_type] for sample in data])\n",
    "    labels = np.array([sample['label']['threat_level'] for sample in data])\n",
    "    \n",
    "    # Add channel dimension for CNN if using spectrograms\n",
    "    if feature_type in ['mel_spectrogram', 'mfcc']:\n",
    "        features = features[..., np.newaxis]\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "# Prepare data with mel-spectrograms\n",
    "print(\"Preparing mel-spectrogram features...\")\n",
    "X_train, y_train = prepare_data(train_data, 'mel_spectrogram')\n",
    "X_val, y_val = prepare_data(val_data, 'mel_spectrogram')\n",
    "X_test, y_test = prepare_data(test_data, 'mel_spectrogram')\n",
    "\n",
    "print(f\"\\nTraining features shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"\\nValidation features shape: {X_val.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"\\nLabel distribution (train):\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  Class {label}: {count:,} samples ({count/len(y_train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "print(\"Class weights (to handle imbalance):\")\n",
    "for cls, weight in class_weight_dict.items():\n",
    "    threat_name = ['BACKGROUND', 'THREAT_CONTEXT', 'THREAT'][cls]\n",
    "    print(f\"  {threat_name} (class {cls}): {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a6ffae",
   "metadata": {},
   "source": [
    "## 6. Build CNN Model for Mel-Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2345251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(input_shape: Tuple, num_classes: int = 3) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Build a CNN model for audio classification.\n",
    "    \n",
    "    Architecture optimized for mel-spectrogram inputs.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # Conv Block 1\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Conv Block 2\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Conv Block 3\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Conv Block 4\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Global pooling and dense layers\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_cnn_model(input_shape=X_train.shape[1:])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fb0ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94219ea8",
   "metadata": {},
   "source": [
    "## 7. Setup Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d69acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for models\n",
    "MODEL_DIR = Path(\"/Users/cococe/Desktop/AUDIOSET METADATA/models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = callbacks.ModelCheckpoint(\n",
    "    filepath=str(MODEL_DIR / 'best_model.keras'),\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "tensorboard = callbacks.TensorBoard(\n",
    "    log_dir=str(MODEL_DIR / 'logs'),\n",
    "    histogram_freq=1\n",
    ")\n",
    "\n",
    "callback_list = [early_stopping, model_checkpoint, reduce_lr, tensorboard]\n",
    "\n",
    "print(\"Callbacks configured:\")\n",
    "print(\"  - Early stopping (patience=15)\")\n",
    "print(\"  - Model checkpoint (save best)\")\n",
    "print(\"  - Learning rate reduction (factor=0.5, patience=5)\")\n",
    "print(\"  - TensorBoard logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fb69b0",
   "metadata": {},
   "source": [
    "## 8. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462c7b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callback_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d460f",
   "metadata": {},
   "source": [
    "## 9. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6671eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Train Loss')\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Model Loss', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].set_title('Model Accuracy', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision\n",
    "axes[1, 0].plot(history.history['precision'], label='Train Precision')\n",
    "axes[1, 0].plot(history.history['val_precision'], label='Val Precision')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Precision')\n",
    "axes[1, 0].set_title('Model Precision', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Recall\n",
    "axes[1, 1].plot(history.history['recall'], label='Train Recall')\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Val Recall')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Recall')\n",
    "axes[1, 1].set_title('Model Recall', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f13d1",
   "metadata": {},
   "source": [
    "## 10. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\\n\")\n",
    "test_results = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"  Loss: {test_results[0]:.4f}\")\n",
    "print(f\"  Accuracy: {test_results[1]:.4f}\")\n",
    "print(f\"  Precision: {test_results[2]:.4f}\")\n",
    "print(f\"  Recall: {test_results[3]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5049ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Classification report\n",
    "class_names = ['BACKGROUND', 'THREAT_CONTEXT', 'THREAT']\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e54dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Normalized confusion matrix\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='RdYlGn',\n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Percentage'})\n",
    "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "plt.title('Normalized Confusion Matrix - Test Set', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036a77c",
   "metadata": {},
   "source": [
    "## 11. Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39cf47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "print(\"\\nPer-Class Performance Metrics:\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Visualize metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    axes[idx].bar(class_names, metrics_df[metric], color=colors)\n",
    "    axes[idx].set_ylim([0, 1.1])\n",
    "    axes[idx].set_ylabel(metric)\n",
    "    axes[idx].set_title(f'{metric} by Class', fontweight='bold')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(metrics_df[metric]):\n",
    "        axes[idx].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d635d75c",
   "metadata": {},
   "source": [
    "## 12. Critical Threat Detection Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4802c949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze false negatives for THREAT class (most critical)\n",
    "threat_indices = np.where(y_test == 2)[0]  # THREAT = class 2\n",
    "threat_predictions = y_pred[threat_indices]\n",
    "\n",
    "false_negatives = np.sum(threat_predictions != 2)\n",
    "true_positives = np.sum(threat_predictions == 2)\n",
    "\n",
    "print(\"\\n=== CRITICAL THREAT DETECTION ANALYSIS ===\")\n",
    "print(f\"\\nTotal THREAT samples in test set: {len(threat_indices)}\")\n",
    "print(f\"Correctly detected: {true_positives} ({true_positives/len(threat_indices)*100:.1f}%)\")\n",
    "print(f\"Missed (False Negatives): {false_negatives} ({false_negatives/len(threat_indices)*100:.1f}%)\")\n",
    "\n",
    "if false_negatives > 0:\n",
    "    print(f\"\\nâš ï¸  WARNING: {false_negatives} THREAT samples were not detected!\")\n",
    "    print(\"These represent critical failures in the anti-poaching system.\")\n",
    "    \n",
    "    # Show what threats were misclassified as\n",
    "    fn_indices = threat_indices[threat_predictions != 2]\n",
    "    fn_predictions = threat_predictions[threat_predictions != 2]\n",
    "    \n",
    "    print(\"\\nMisclassifications:\")\n",
    "    unique_preds, counts = np.unique(fn_predictions, return_counts=True)\n",
    "    for pred, count in zip(unique_preds, counts):\n",
    "        print(f\"  THREAT misclassified as {class_names[pred]}: {count} times\")\n",
    "else:\n",
    "    print(\"\\nâœ… Perfect THREAT detection! No false negatives.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c65e84",
   "metadata": {},
   "source": [
    "## 13. Save Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d234d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_path = MODEL_DIR / 'threat_detection_final.keras'\n",
    "model.save(final_model_path)\n",
    "print(f\"\\nâœ… Final model saved to: {final_model_path}\")\n",
    "\n",
    "# Save model configuration\n",
    "model_config = {\n",
    "    'model_type': 'CNN',\n",
    "    'feature_type': 'mel_spectrogram',\n",
    "    'input_shape': list(X_train.shape[1:]),\n",
    "    'num_classes': 3,\n",
    "    'class_names': class_names,\n",
    "    'preprocessing': config,\n",
    "    'test_accuracy': float(test_results[1]),\n",
    "    'test_precision': float(test_results[2]),\n",
    "    'test_recall': float(test_results[3]),\n",
    "    'class_metrics': metrics_df.to_dict('records')\n",
    "}\n",
    "\n",
    "config_path = MODEL_DIR / 'model_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Model configuration saved to: {config_path}\")\n",
    "\n",
    "# Export to TensorFlow Lite for edge deployment\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_path = MODEL_DIR / 'threat_detection.tflite'\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"âœ… TensorFlow Lite model saved to: {tflite_path}\")\n",
    "print(f\"   Model size: {len(tflite_model) / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee9942",
   "metadata": {},
   "source": [
    "## 14. Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b42d8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test real-time inference\n",
    "def predict_threat(model, audio_features: np.ndarray) -> Dict:\n",
    "    \"\"\"\n",
    "    Predict threat level from audio features.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        audio_features: Preprocessed audio features\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with prediction results\n",
    "    \"\"\"\n",
    "    # Add batch dimension if needed\n",
    "    if audio_features.ndim == 3:\n",
    "        audio_features = np.expand_dims(audio_features, axis=0)\n",
    "    \n",
    "    # Get prediction\n",
    "    probabilities = model.predict(audio_features, verbose=0)[0]\n",
    "    predicted_class = np.argmax(probabilities)\n",
    "    confidence = probabilities[predicted_class]\n",
    "    \n",
    "    return {\n",
    "        'threat_level': int(predicted_class),\n",
    "        'threat_name': class_names[predicted_class],\n",
    "        'confidence': float(confidence),\n",
    "        'probabilities': {\n",
    "            name: float(prob) for name, prob in zip(class_names, probabilities)\n",
    "        },\n",
    "        'alert_rangers': predicted_class >= 2  # Alert if THREAT detected\n",
    "    }\n",
    "\n",
    "# Test on random samples\n",
    "print(\"\\n=== INFERENCE DEMO ===\")\n",
    "print(\"Testing on 5 random samples from test set...\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    idx = np.random.randint(0, len(X_test))\n",
    "    sample_features = X_test[idx]\n",
    "    true_label = y_test[idx]\n",
    "    \n",
    "    result = predict_threat(model, sample_features)\n",
    "    \n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  True Label: {class_names[true_label]}\")\n",
    "    print(f\"  Predicted: {result['threat_name']} (confidence: {result['confidence']:.1%})\")\n",
    "    print(f\"  Alert Rangers: {'ðŸš¨ YES' if result['alert_rangers'] else 'âœ“ No'}\")\n",
    "    print(f\"  Probabilities:\")\n",
    "    for name, prob in result['probabilities'].items():\n",
    "        print(f\"    {name}: {prob:.1%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1415d5b",
   "metadata": {},
   "source": [
    "## 15. Deployment Recommendations\n",
    "\n",
    "### Model Performance Summary\n",
    "- The model has been trained and evaluated on 6,734 audio samples\n",
    "- Three-tier threat classification system ready for ranger notification\n",
    "- TensorFlow Lite model exported for edge deployment\n",
    "\n",
    "### Deployment Considerations\n",
    "\n",
    "1. **Real-Time Processing**:\n",
    "   - Model processes 10-second audio clips\n",
    "   - Inference time: ~10-50ms on modern hardware\n",
    "   - Suitable for continuous monitoring\n",
    "\n",
    "2. **Alert System**:\n",
    "   - THREAT (Level 2): Immediate ranger alert\n",
    "   - THREAT_CONTEXT (Level 1): Log and monitor\n",
    "   - BACKGROUND (Level 0): No action required\n",
    "\n",
    "3. **Edge Deployment**:\n",
    "   - TensorFlow Lite model: Lightweight for Raspberry Pi/edge devices\n",
    "   - Low power consumption for solar-powered field deployment\n",
    "   - Offline operation capability\n",
    "\n",
    "4. **Monitoring and Improvement**:\n",
    "   - Log all predictions with timestamps and GPS coordinates\n",
    "   - Collect ranger feedback for continuous improvement\n",
    "   - Retrain model periodically with new data\n",
    "   - Monitor false positive/negative rates\n",
    "\n",
    "5. **Critical Metrics to Watch**:\n",
    "   - THREAT recall (minimize missed threats)\n",
    "   - False positive rate (avoid alert fatigue)\n",
    "   - Model drift over time\n",
    "\n",
    "### Next Steps\n",
    "1. Deploy TensorFlow Lite model to field devices\n",
    "2. Integrate with ranger communication system\n",
    "3. Set up monitoring dashboard\n",
    "4. Conduct field testing and validation\n",
    "5. Implement feedback loop for model improvement"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
